<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  
  
  
    <meta name="description" content="Ain&#39;t no mountain high enough">
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <title>
    pytorch预备知识 |
    
    Blog_JosephLZD</title>
  
    <link rel="shortcut icon" href="/favicon.ico">
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<script src="/js/pace.min.js"></script>

<meta name="generator" content="Hexo 4.1.1"></head>

<body>
<main class="content">
  <section class="outer">
  

<article id="post-pytorch预备知识" class="article article-type-post" itemscope itemprop="blogPost" data-scroll-reveal>
  
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      pytorch预备知识
    </h1>
  
  




      </header>
    

    
      <div class="article-meta">
        <a href="/2020/02/02/pytorch%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86/" class="article-date">
  <time datetime="2020-02-02T14:37:00.000Z" itemprop="datePublished">2020-02-02</time>
</a>
        
      </div>
    

    
      
    <div class="tocbot"></div>





    

    <div class="article-entry" itemprop="articleBody">
      


      

      
        <p>《动手学深度学习》学习笔记day1<br>&lt;!—more—&gt;</p>
<h1 id="Tensor"><a href="#Tensor" class="headerlink" title="Tensor"></a>Tensor</h1><blockquote>
<p>“tensor”这个单词一般可译作“张量”，张量可以看作是一个多维数组。标量可以看作是0维张量，向量可以看作1维张量，矩阵可以看作是二维张量。</p>
</blockquote>
<h3 id="介绍Tensor的创建方法"><a href="#介绍Tensor的创建方法" class="headerlink" title="介绍Tensor的创建方法"></a>介绍Tensor的创建方法</h3><p>torch.empty(5, 3) ； 创建空张量<br>torch.rand(5, 3) ；<br>torch.tensor([5.5, 3]) ； 直接根据数据创建<br>x.size() 或者 x.shape() ；获取张量的形状</p>
<h3 id="索引"><a href="#索引" class="headerlink" title="索引"></a>索引</h3><p>需要注意的是：索引出来的结果与原数据<strong>共享内存</strong>，也即修改一个，另一个会跟着修改。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">y = x[<span class="number">0</span>, :]</span><br><span class="line">y += <span class="number">1</span></span><br><span class="line">print(y)</span><br><span class="line">print(x[<span class="number">0</span>, :]) <span class="comment"># 源tensor也被改了</span></span><br></pre></td></tr></table></figure>
<h3 id="改变形状"><a href="#改变形状" class="headerlink" title="改变形状"></a>改变形状</h3><p>用view()来改变Tensor的形状：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">y = x.view(<span class="number">15</span>) <span class="comment">#把x规模改成15长度的一维向量</span></span><br><span class="line">z = x.view(<span class="number">-1</span>, <span class="number">5</span>) <span class="comment">#-1所指的维度可以根据其他维度的值推出来，比如是15个数，所以这里-1指的维度即行数自动设为3</span></span><br><span class="line">print(x.size() , y.size(), z.size())</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([<span class="number">5</span>, <span class="number">3</span>]) torch.Size([<span class="number">15</span>]) torch.Size([<span class="number">3</span>, <span class="number">5</span>])</span><br></pre></td></tr></table></figure>

<p><strong>注意</strong>：就像索引一样，view取出的结果也是共享内存的，即后续改变同步。<br>所以，如果我们想返回一个真正新的副本（不共享data内存），该怎么办呢？用clone()函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x_cp = x.clone().view(<span class="number">15</span>)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>使用clone还有一个好处是会被记录在计算图中，即梯度回传到副本时也会传到源Tensor。</p>
</blockquote>
<h3 id="item-函数"><a href="#item-函数" class="headerlink" title="item()函数"></a>item()函数</h3><p>item()函数可以将一个张量的内容转换成一个python number：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">1</span>)</span><br><span class="line">print(x)</span><br><span class="line">print(x.item())</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([<span class="number">2.3466</span>])</span><br><span class="line"><span class="number">2.3466382026672363</span></span><br></pre></td></tr></table></figure>

<h3 id="线性代数"><a href="#线性代数" class="headerlink" title="线性代数"></a>线性代数</h3><table>
<thead>
<tr>
<th>函数</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>trace</td>
<td>对角线元素之和（矩阵的迹）</td>
</tr>
<tr>
<td>diag</td>
<td>对角线元素</td>
</tr>
<tr>
<td>mm/bmm</td>
<td>矩阵乘法，batch的矩阵乘法</td>
</tr>
<tr>
<td>inverse</td>
<td>求逆矩阵</td>
</tr>
<tr>
<td>PyTorch中的Tensor支持超过一百种操作，包括转置、索引、切片、数学运算、线性代数、随机数等等，更多操作见官方文档：<a href="https://pytorch.org/docs/stable/tensors.html" target="_blank" rel="noopener">https://pytorch.org/docs/stable/tensors.html</a></td>
<td></td>
</tr>
</tbody></table>
<h3 id="广播机制"><a href="#广播机制" class="headerlink" title="广播机制"></a>广播机制</h3><p>前面我们看到如何对两个形状相同的Tensor做按元素运算。当对两个形状不同的Tensor按元素运算时，可能会触发广播（broadcasting）机制：<strong>先适当复制元素使这两个Tensor形状相同后再按元素运算</strong>。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = torch.arange(<span class="number">1</span>,<span class="number">3</span>).view(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">print(x)</span><br><span class="line">y = torch.arange(<span class="number">1</span>,<span class="number">4</span>).view(<span class="number">3</span>,<span class="number">1</span>)</span><br><span class="line">print(y)</span><br><span class="line">print(x+y)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tensor([[<span class="number">1</span>, <span class="number">2</span>]])</span><br><span class="line">tensor([[<span class="number">1</span>],</span><br><span class="line">        [<span class="number">2</span>],</span><br><span class="line">        [<span class="number">3</span>]])</span><br><span class="line">tensor([[<span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">        [<span class="number">4</span>, <span class="number">5</span>]])</span><br></pre></td></tr></table></figure>

<p>由于x和y分别是1行2列和3行1列的矩阵，如果要计算x + y，那么x中第一行的2个元素被广播（复制）到了第二行和第三行，而y中第一列的3个元素被广播（复制）到了第二列。如此，就可以对2个3行2列的矩阵按元素相加。</p>
<h3 id="tensor和numpy数组转换"><a href="#tensor和numpy数组转换" class="headerlink" title="tensor和numpy数组转换"></a>tensor和numpy数组转换</h3><p>1.使用<strong>numpy()</strong>将tensor转换成numpy数组：<br><em>注意：共享内存</em></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a = torch.ones(<span class="number">5</span>)</span><br><span class="line">b = a.numpy(a)</span><br><span class="line">print(a,b)</span><br><span class="line">a+=<span class="number">1</span></span><br><span class="line">print(a,b)</span><br><span class="line">b+=<span class="number">1</span></span><br><span class="line">print(a,b)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensor([<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>]) [<span class="number">1.</span> <span class="number">1.</span> <span class="number">1.</span> <span class="number">1.</span> <span class="number">1.</span>]</span><br><span class="line">tensor([<span class="number">2.</span>, <span class="number">2.</span>, <span class="number">2.</span>, <span class="number">2.</span>, <span class="number">2.</span>]) [<span class="number">2.</span> <span class="number">2.</span> <span class="number">2.</span> <span class="number">2.</span> <span class="number">2.</span>]</span><br><span class="line">tensor([<span class="number">3.</span>, <span class="number">3.</span>, <span class="number">3.</span>, <span class="number">3.</span>, <span class="number">3.</span>]) [<span class="number">3.</span> <span class="number">3.</span> <span class="number">3.</span> <span class="number">3.</span> <span class="number">3.</span>]</span><br></pre></td></tr></table></figure>

<p>2.使用<strong>from_numpy()</strong>将numpy数组转换成tensor：<br><strong>注意：共享内存</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">a = np.ones(<span class="number">5</span>)</span><br><span class="line">b = torch.from_numpy(a)</span><br><span class="line">print(a,b)</span><br></pre></td></tr></table></figure>
<p>3.使用torch.tensor()也可将numpy数组转换成tensor，但此时不再共享内存。</p>
<h2 id="自动梯度求导"><a href="#自动梯度求导" class="headerlink" title="自动梯度求导"></a>自动梯度求导</h2><h3 id="requires-grad"><a href="#requires-grad" class="headerlink" title="requires_grad"></a>requires_grad</h3><p>创建一个Tensor并设置requires_grad=True:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = torch.ones(<span class="number">2</span>, <span class="number">2</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">print(x)</span><br><span class="line">print(x.grad_fn)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensor([[<span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>]], requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="literal">None</span></span><br></pre></td></tr></table></figure>

<p>通过.requires_grad_()来用in-place的方式改变requires_grad属性.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a.requires_grad_(<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<h3 id="grad-fn"><a href="#grad-fn" class="headerlink" title="grad_fn"></a>grad_fn</h3><p>Function是另外一个很重要的类。Tensor和Function互相结合就可以构建一个记录有整个计算过程的有向无环图（DAG）。每个Tensor都有一个.grad_fn属性，该属性即创建该Tensor的Function, 就是说该Tensor是不是通过某些运算得到的，若是，则grad_fn返回一个与这些运算相关的对象，否则是None。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">y = x+<span class="number">2</span></span><br><span class="line">print(y)</span><br><span class="line">print(y.grad_fn)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensor([[<span class="number">3.</span>, <span class="number">3.</span>],</span><br><span class="line">        [<span class="number">3.</span>, <span class="number">3.</span>]], grad_fn=&lt;AddBackward&gt;)</span><br><span class="line">&lt;AddBackward object at <span class="number">0x1100477b8</span>&gt;</span><br></pre></td></tr></table></figure>

<p>注意x是直接创建的，所以它没有grad_fn, 而y是通过一个加法操作创建的，所以它有一个为<AddBackward>的grad_fn。<br>调用.is_leaf来看在无向图中是否是叶子结点：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(x.is_leaf, y.is_leaf) <span class="comment">#True False</span></span><br></pre></td></tr></table></figure>

<h3 id="反向传播backward"><a href="#反向传播backward" class="headerlink" title="反向传播backward()"></a>反向传播backward()</h3><p>首先注意“雅克比矩阵”的定义，理解链式法则在雅可比矩阵上的计算：<br><img src="https://imgvideoforblog.oss-cn-shanghai.aliyuncs.com/pytorch%E5%9B%BE1.png" alt="Alt"><br>backward()之后，则可调用.grad()，结合该自变量的数值和求导结果，查看该变量的梯度值，比如求导出来是2x，而x1=1，那么x1.grad()=2<em>1=2.<br>*</em>注意：我们不允许张量对张量求导（否则结果的维度太复杂），只允许标量对张量求导，求导结果是和自变量同形的张量。（这个结论可以从上图中看出来）**</p>
<blockquote>
<p>为什么在y.backward()时，如果y是标量，则不需要为backward()传入任何参数；否则，需要传入一个与y同形的Tensor? 简单来说就是为了避免向量（甚至更高维张量）对张量求导，而转换成标量对张量求导。举个例子，假设形状为 m x n 的矩阵 X 经过运算得到了 p x q 的矩阵 Y，Y 又经过运算得到了 s x t 的矩阵 Z。那么按照前面讲的规则，dZ/dY 应该是一个 s x t x p x q 四维张量，dY/dX 是一个 p x q x m x n的四维张量。问题来了，怎样反向传播？怎样将两个四维张量相乘？？？这要怎么乘？？？就算能解决两个四维张量怎么乘的问题，四维和三维的张量又怎么乘？导数的导数又怎么求，这一连串的问题，感觉要疯掉…… 为了避免这个问题，我们不允许张量对张量求导，只允许标量对张量求导，求导结果是和自变量同形的张量。所以必要时我们要把张量通过将所有张量的元素加权求和的方式转换为标量，举个例子，假设y由自变量x计算而来，w是和y同形的张量，则y.backward(w)的含义是：先计算l = torch.sum(y * w)，则l是个标量，然后求l对自变量x的导数。 </p>
</blockquote>
<p>实际例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = <span class="number">2</span> * x</span><br><span class="line">z = y.view(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">print(z)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([[<span class="number">2.</span>, <span class="number">4.</span>],</span><br><span class="line">        [<span class="number">6.</span>, <span class="number">8.</span>]], grad_fn=&lt;ViewBackward&gt;)</span><br></pre></td></tr></table></figure>

<p>现在 z 不是一个标量，所以在调用backward时需要传入一个和z同形的权重向量进行加权求和得到一个标量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">v = torch.tensor([<span class="number">1</span>,<span class="number">0.1</span>],[<span class="number">0.01</span>,<span class="number">0.001</span>]] )</span><br><span class="line">z.backward(v) <span class="comment">#由于z不是标量，backward传入同形向量v</span></span><br><span class="line">print(x.grad)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([<span class="number">2.0000</span>, <span class="number">0.2000</span>, <span class="number">0.0200</span>, <span class="number">0.0020</span>])</span><br></pre></td></tr></table></figure>

<p>注意，x.grad是和x同形的张量。</p>
<h3 id="中断梯度追踪"><a href="#中断梯度追踪" class="headerlink" title="中断梯度追踪"></a>中断梯度追踪</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor(<span class="number">1.0</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">y1 = x ** <span class="number">2</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad(): <span class="comment">#这里不要梯度追踪</span></span><br><span class="line">    y2 = x**<span class="number">3</span></span><br><span class="line">y3 = y1 + y2</span><br><span class="line"></span><br><span class="line">print(x.requires_grad)      <span class="comment"># True</span></span><br><span class="line">print(y1, y1.requires_grad) <span class="comment"># True</span></span><br><span class="line">print(y2, y2.requires_grad) <span class="comment"># False</span></span><br><span class="line">print(y3, y3.requires_grad) <span class="comment"># True</span></span><br></pre></td></tr></table></figure>
<p>可以看到，上面的y2是没有grad_fn而且y2.requires_grad=False的，而y3是有grad_fn的。如果我们将y3对x求梯度的话会是多少呢？</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y3.backward()</span><br><span class="line">print(x.grad)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor(<span class="number">2.</span>)</span><br></pre></td></tr></table></figure>

<p>注意，y3=y1+y2=x<strong>2+x</strong>3，x=1时x.grad应该为5，但由于y2中断梯度追踪，所以只看y1即x*<em>2，求导出来=2x=2</em>1=5.</p>
<h3 id="修改数值但不希望影响grad值"><a href="#修改数值但不希望影响grad值" class="headerlink" title="修改数值但不希望影响grad值"></a>修改数值但不希望影响grad值</h3><p>刚刚我提到，x.grad值是要结合求导结果和x当前数值的，但是如果只想修改数值而不想影响grad值的话，就调用.data来进行运算。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">x = torch.ones(<span class="number">1</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">print(x,data) </span><br><span class="line">print(x.data.requires_grad) <span class="comment">#.data独立于计算图之外</span></span><br><span class="line">y = <span class="number">2</span>*x</span><br><span class="line">x.data *= <span class="number">100</span> <span class="comment">#x.data此时=100</span></span><br><span class="line"></span><br><span class="line">y.backward() <span class="comment">#y是标量，可不传参</span></span><br><span class="line">print(x) <span class="comment">#因为.data更改过，这里显示的是更改过后的值</span></span><br><span class="line">print(x.grad) <span class="comment">#但对于grad值，仍基于原来的x=1来算</span></span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor([<span class="number">1.</span>])</span><br><span class="line"><span class="literal">False</span></span><br><span class="line">tensor([<span class="number">100.</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">tensor([<span class="number">2.</span>])</span><br></pre></td></tr></table></figure>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/02/02/pytorch%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86/" data-id="ck654w0zt00004y6cenrfbj4g"
         class="article-share-link">Share</a>
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag">动手学深度学习</a></li></ul>

    </footer>

  </div>

  
    
  <nav class="article-nav">
    
    
      <a href="/2020/01/14/DeepLearning/" class="article-nav-link">
        <strong class="article-nav-caption">Olde posts</strong>
        <div class="article-nav-title">DeepLearning</div>
      </a>
    
  </nav>


  

  
    
  <div class="gitalk" id="gitalk-container"></div>
  
<link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">

  
<script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>

  
<script src="https://cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.min.js"></script>

  <script type="text/javascript">
    var gitalk = new Gitalk({
      clientID: 'dd9dd0a01eb6aea2b4b2',
      clientSecret: 'f10df8d5a64e14405c16d484ff17f526f1ff2c21',
      repo: 'gitalk_blog',
      owner: 'JosephLZD',
      admin: ['JosephLZD'],
      // id: location.pathname,      // Ensure uniqueness and length less than 50
      id: md5(location.pathname),
      distractionFreeMode: false,  // Facebook-like distraction free mode
      pagerDirection: 'last'
    })

  gitalk.render('gitalk-container')
  </script>

  

</article>



</section>
  <footer class="footer">
  <div class="outer">
    <div class="float-right">
      <ul class="list-inline">
  
    <li><i class="fe fe-smile-alt"></i> <span id="busuanzi_value_site_uv"></span></li>
  
</ul>
    </div>
    <ul class="list-inline">
      <li>&copy; 2020 Blog_JosephLZD</li>
      <li>Powered by <a href="http://hexo.io/" target="_blank">Hexo</a></li>
    </ul>
  </div>
</footer>

</main>

<aside class="sidebar sidebar-specter">
  
    <button class="navbar-toggle"></button>
<nav class="navbar">
  
    <div class="logo">
      <a href="/"><img src="/images/hexo.svg" alt="Blog_JosephLZD"></a>
    </div>
  
  <ul class="nav nav-main">
    
      <li class="nav-item">
        <a class="nav-item-link" href="/">Home</a>
      </li>
    
      <li class="nav-item">
        <a class="nav-item-link" href="/archives">Archive</a>
      </li>
    
      <li class="nav-item">
        <a class="nav-item-link" href="/gallery">Gallery</a>
      </li>
    
      <li class="nav-item">
        <a class="nav-item-link" href="/about">About</a>
      </li>
    
    <li class="nav-item">
      <a class="nav-item-link nav-item-search" title="搜索">
        <i class="fe fe-search"></i>
        Search
      </a>
    </li>
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      <div class="totop" id="totop">
  <i class="fe fe-rocket"></i>
</div>
    </li>
    <li class="nav-item">
      
        <a class="nav-item-link" target="_blank" href="/atom.xml" title="RSS Feed">
          <i class="fe fe-feed"></i>
        </a>
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
  </aside>
  
<script src="/js/jquery-2.0.3.min.js"></script>


<script src="/js/jquery.justifiedGallery.min.js"></script>


<script src="/js/lazyload.min.js"></script>


<script src="/js/busuanzi-2.3.pure.min.js"></script>


  
<script src="/fancybox/jquery.fancybox.min.js"></script>




  
<script src="/js/tocbot.min.js"></script>

  <script>
    // Tocbot_v4.7.0  http://tscanlin.github.io/tocbot/
    tocbot.init({
      tocSelector: '.tocbot',
      contentSelector: '.article-entry',
      headingSelector: 'h1, h2, h3, h4, h5, h6',
      hasInnerContainers: true,
      scrollSmooth: true,
      positionFixedSelector: '.tocbot',
      positionFixedClass: 'is-position-fixed',
      fixedSidebarOffset: 'auto',
    });
  </script>



<script src="/js/ocean.js"></script>


</body>
</html>