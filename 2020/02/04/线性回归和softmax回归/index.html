<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  
  
  
    <meta name="description" content="Ain&#39;t no mountain high enough">
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <title>
    线性回归和softmax回归 |
    
    Blog_JosephLZD</title>
  
    <link rel="shortcut icon" href="/favicon.ico">
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<script src="/js/pace.min.js"></script>

<meta name="generator" content="Hexo 4.1.1"></head>

<body>
<main class="content">
  <section class="outer">
  

<article id="post-线性回归和softmax回归" class="article article-type-post" itemscope itemprop="blogPost" data-scroll-reveal>
  
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      线性回归和softmax回归
    </h1>
  
  




      </header>
    

    
      <div class="article-meta">
        <a href="/2020/02/04/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E5%92%8Csoftmax%E5%9B%9E%E5%BD%92/" class="article-date">
  <time datetime="2020-02-04T08:17:53.000Z" itemprop="datePublished">2020-02-04</time>
</a>
        
      </div>
    

    
      
    <div class="tocbot"></div>





    

    <div class="article-entry" itemprop="articleBody">
      


      

      
        <p>《动手学深度学习pytorch版》学习笔记-深度学习基础</p>
<a id="more"></a>
<h1 id="深度学习基础"><a href="#深度学习基础" class="headerlink" title="深度学习基础"></a>深度学习基础</h1><h2 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h2><p>以面积、房龄来预测房屋价格的例子，线性函数、最小二乘法计算误差、最小化误差的目标函数优化。</p>
<blockquote>
<p>当模型和损失函数形式较为简单时，上面的误差最小化问题的解可以直接用公式表达出来。这类解叫作<strong>解析解（analytical solution）</strong>。本节使用的线性回归和平方误差刚好属于这个范畴。然而，大多数深度学习模型并没有解析解，只能通过优化算法有限次迭代模型参数来尽可能降低损失函数的值。这类解叫作<strong>数值解（numerical solution）</strong>。<br>在求数值解的优化算法中，小批量<strong>随机梯度下降</strong>（mini-batch stochastic gradient descent）在深度学习中被广泛使用。它的算法很简单：先选取一组模型参数的初始值，如随机选取；接下来对参数进行多次迭代，使每次迭代都可能降低损失函数的值。在每次迭代中，先随机均匀采样一个由固定数目训练数据样本所组成的小批量（mini-batch）BB，然后求小批量中数据样本的平均损失有关模型参数的导数（梯度），最后用此结果与预先设定的一个正数的乘积作为模型参数在本次迭代的减小量，这个正数也就是learning rate。</p>
</blockquote>
<p>在训练本节讨论的线性回归模型的过程中，模型的每个参数将作如下迭代：<br><img src="https://imgvideoforblog.oss-cn-shanghai.aliyuncs.com/20200204%E5%9B%BE1.png" alt="Alt"><br>在上式中，∣B∣ 代表每个小批量中的样本个数（批量大小，batch size），η称作学习率（learning rate）并取正数。需要强调的是，这里的批量大小和学习率的值是人为设定的，并不是通过模型训练学出的，因此叫作超参数（hyperparameter）。我们通常所说的“调参”指的正是调节超参数，例如通过反复试错来找到超参数合适的值。在少数情况下，超参数也可以通过模型训练学出，本书对此类情况不做讨论。<br><strong>这里跟我以往理解的不同在于它梯度下降中，是只对一部分数据样本进行导数计算并且求平均值，所以要求和并除以小批量中的样本个数|B|。</strong></p>
<p>线性回归是一个单层神经网络。输出层中负责计算 o的单元又叫神经元。在线性回归中，<strong>输出层中的神经元和输入层中各个输入完全连接</strong>。因此，这里的输出层又叫<strong>全连接层</strong>（fully-connected layer）或稠密层（dense layer）<br><Br></p>
<h2 id="线性回归从零开始实现"><a href="#线性回归从零开始实现" class="headerlink" title="线性回归从零开始实现"></a>线性回归从零开始实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">matplotlib inline</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> IPython <span class="keyword">import</span> display</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> random</span><br></pre></td></tr></table></figure>
<h3 id="生成数据集"><a href="#生成数据集" class="headerlink" title="生成数据集"></a>生成数据集</h3><p>先自己来生成标签，加入噪声项，其噪声项服从均值为0、标准差为0.01的正态分布。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">num_inputs = <span class="number">2</span></span><br><span class="line">num_examples = <span class="number">1000</span></span><br><span class="line">true_w = [<span class="number">2</span>, <span class="number">-3.4</span>]</span><br><span class="line">true_b = <span class="number">4.2</span></span><br><span class="line">features = torch.randn(num_examples, num_inputs, dtype=torch.float32)</span><br><span class="line">labels = true_w[<span class="number">0</span>] * features[:,<span class="number">0</span>] + true_w[<span class="number">1</span>] * features[:,<span class="number">1</span>] + true_b</span><br><span class="line">labels += torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=labels.size()), dtype=torch.float32)  <span class="comment">#这里学习如何生成服从一定的高斯分布的数据，包括设定生成数据的规模。</span></span><br></pre></td></tr></table></figure>
<h3 id="读取数据"><a href="#读取数据" class="headerlink" title="读取数据"></a>读取数据</h3><p>先定义一个data_iter函数，用于随机取出一个batch的训练数据的特征和对应标签。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_iter</span><span class="params">(batch_size, features, labels)</span>:</span></span><br><span class="line">    num_examples = len(features)</span><br><span class="line">    indices = list(range(num_examples)) <span class="comment">#索引数组</span></span><br><span class="line">    random.shuffle(indices) <span class="comment">#索引数组打乱顺序，表示每个batch中的数据是随机抽样的。</span></span><br><span class="line">    <span class="comment">#读一个batch的数据</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_examples, batch_size):</span><br><span class="line">    j = torch.LongTensor(indices[i: min(i+batch_size, num_examples)]) <span class="comment">#去这个索引数组中取一个batch长度的索引，作为本次batch所要抽样的下标，由于最后一次可能不够一个batch，所以有min函数的逻辑。</span></span><br><span class="line">    <span class="keyword">yield</span> features.index_select(<span class="number">0</span>,j), labels.index_select(<span class="number">0</span>,j) <span class="comment">#0代表第一维，j是本个batch的索引张量</span></span><br></pre></td></tr></table></figure>
<p>这里的yield很重要！！首先其实可以粗暴地理解为return，但其特点在于，下一次调用该函数，可以分多次return，每次return紧接下一次return，是一种“生成器”。</p>
<h3 id="初始化模型参数"><a href="#初始化模型参数" class="headerlink" title="初始化模型参数"></a>初始化模型参数</h3><p>将权重初始化成均值为0、标准差为0.01的正态随机数，偏差则初始化成0。 </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">w = torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>, (<span class="number">2</span>,<span class="number">1</span>)), dtype=torch.float32)</span><br><span class="line">b = torch.zeros(<span class="number">1</span>, dtype=torch.float32)</span><br><span class="line">w.requires_grad_(<span class="literal">True</span>)</span><br><span class="line">b.requires_grad_(<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h3 id="定义模型"><a href="#定义模型" class="headerlink" title="定义模型"></a>定义模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linreg</span><span class="params">(X, w, b)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> torch.mm(X,w) + b <span class="comment">#矩阵乘法</span></span><br></pre></td></tr></table></figure>
<h3 id="定义损失函数"><a href="#定义损失函数" class="headerlink" title="定义损失函数"></a>定义损失函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">squared_loss</span><span class="params">(y_hat, y)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (y_hat - y.view(y_hat.size()) )**<span class="number">2</span> /<span class="number">2</span>  <span class="comment">#最小二乘法误差，这里注意规模统一</span></span><br></pre></td></tr></table></figure>
<h3 id="定义优化算法"><a href="#定义优化算法" class="headerlink" title="定义优化算法"></a>定义优化算法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sgd</span><span class="params">(params, lr, batch_size)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">        param.data -= lr * param.grad / batch_size</span><br></pre></td></tr></table></figure>

<h3 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">lr = <span class="number">0.03</span></span><br><span class="line">num_epochs = <span class="number">3</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):  <span class="comment"># 训练模型一共需要num_epochs个迭代周期</span></span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter(batch_size, features, labels): <span class="comment">#提取一个batch的数据</span></span><br><span class="line">        l = squared_loss(linreg(X,w,b),y).sum() <span class="comment">## l是有关小批量X和y的损失，注意由于loss是向量而非标量，而我们只允许标量对向量求导，所以用sum求和成标量！</span></span><br><span class="line">        l.backward()</span><br><span class="line">        sgd([w,b],lr,batch_size) <span class="comment">#小批量梯度下降，只用这一个batch的数据来更新一次参数</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">#为了下一次梯度下降，记得梯度清零</span></span><br><span class="line">        w.grad.data.zero_()</span><br><span class="line">        b.grad.data.zero_()</span><br><span class="line">    train_l = squared_loss(linreg(features, w, b), labels) <span class="comment">#对整体进行损失计算</span></span><br><span class="line">    print(<span class="string">'epoch %d, loss %f'</span> % (epoch + <span class="number">1</span>, train_l.mean().item())) <span class="comment">#item函数将只含一个数的张量转换为一个数形式</span></span><br></pre></td></tr></table></figure>
<p>上面这段代码需要好好理解一下，在于之前定义的data_iter函数其实会把<strong>整个数据集分成的各个batch</strong>都返回出来，但由于是yield，所以该函数变成了生成器，所以每个batch会随着for语句即for X, y in data_iter(batch_size, features, labels):一次一次地返回。<br>所以，每个epoch仍是以batch为单位遍历了整个数据集并且进行了小批量梯度下降。</p>
<p>对于yield，以下是网上的解释：</p>
<blockquote>
<p>当你的for第一次调用函数的时候，它生成一个生成器，并且在你的函数中运行该循环，直到它生成第一个值。然后每次调用都会运行循环并且返回下一个值，直到没有值返回为止。</p>
</blockquote>
<h3 id="查看结果"><a href="#查看结果" class="headerlink" title="查看结果"></a>查看结果</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(true_w, <span class="string">'\n'</span>, w)</span><br><span class="line">print(true_b, <span class="string">'\n'</span>, b)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">2</span>, <span class="number">-3.4</span>] </span><br><span class="line"> tensor([[ <span class="number">1.9998</span>],</span><br><span class="line">        [<span class="number">-3.3998</span>]], requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="number">4.2</span> </span><br><span class="line"> tensor([<span class="number">4.2001</span>], requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<br>

<h2 id="线性回归简洁实现"><a href="#线性回归简洁实现" class="headerlink" title="线性回归简洁实现"></a>线性回归简洁实现</h2><h3 id="生成数据集-1"><a href="#生成数据集-1" class="headerlink" title="生成数据集"></a>生成数据集</h3><p>和上节一致</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">num_inputs = <span class="number">2</span></span><br><span class="line">num_examples = <span class="number">1000</span></span><br><span class="line">true_w = [<span class="number">2</span>, <span class="number">-3.4</span>]</span><br><span class="line">true_b = <span class="number">4.2</span></span><br><span class="line">features = torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">1</span>, (num_examples, num_inputs)), dtype=torch.float)</span><br><span class="line">labels = true_w[<span class="number">0</span>] * features[:, <span class="number">0</span>] + true_w[<span class="number">1</span>] * features[:, <span class="number">1</span>] + true_b</span><br><span class="line">labels += torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=labels.size()), dtype=torch.float)</span><br></pre></td></tr></table></figure>

<h3 id="读取数据-1"><a href="#读取数据-1" class="headerlink" title="读取数据"></a>读取数据</h3><p>PyTorch提供了data包来读取数据。由于data常用作变量名，我们将导入的data模块用Data代替。在每一次迭代中，我们将随机读取包含10个数据样本的小批量。<br>用到了Data.TensorDataset和Data.DataLoader函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> Data</span><br><span class="line">batch_size = <span class="number">10</span></span><br><span class="line"><span class="comment">#将训练数据的特征和标签组合</span></span><br><span class="line">dataset = Data.TensorDataset(features, labels)</span><br><span class="line"><span class="comment">#随机读取小批量</span></span><br><span class="line">data_iter = Data.DataLoader(dataset, batch_size, shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<h3 id="定义模型-1"><a href="#定义模型-1" class="headerlink" title="定义模型"></a>定义模型</h3><p>首先，导入torch.nn模块。实际上，“nn”是neural networks（神经网络）的缩写。顾名思义，该模块定义了大量神经网络的层。之前我们已经用过了autograd，而nn就是利用autograd来定义模型。nn的核心数据结构是<strong>Module，它是一个抽象概念，既可以表示神经网络中的某个层（layer），也可以表示一个包含很多层的神经网络。</strong><br>在实际使用中，最常见的做法是继承nn.Module，撰写自己的网络/层。<em>一个nn.Module实例应该包含一些层以及返回输出的前向传播（forward）方法</em>。下面先来看看如何用nn.Module实现一个线性回归模型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LinearNet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n_feature)</span>:</span></span><br><span class="line">        super(LinearNet, self).__init__()</span><br><span class="line">        self.linear = nn.Linear(n_feature,<span class="number">1</span>)</span><br><span class="line">    <span class="comment">#forward 定义前向传播</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,x)</span>:</span></span><br><span class="line">        y = self.linear(x)</span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line">net = LinearNet(num_inputs)</span><br><span class="line">print(net)<span class="comment"># 使用print可以打印出网络的结构</span></span><br></pre></td></tr></table></figure>
<p>补充：nn.Linear(a,b)，意思是：<strong>对于单个样本来说，a是输入数据的维度，b是输出数据的维度。</strong><br>Linear函数会<em>内置为其设置好weight和bias</em>。<br>init函数相当于构造函数，把要传入的Linear的维度给设好，forward函数说是设置前向传播，其实就是把该线性函数的<strong>输入和输出</strong>设置好。</p>
<p>输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">LinearNet(</span><br><span class="line">  (linear): Linear(in_features=<span class="number">2</span>, out_features=<span class="number">1</span>, bias=<span class="literal">True</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>注意：torch.nn仅支持输入一个batch的样本不支持单个样本输入，如果只有单个样本，可使用input.unsqueeze(0)来添加一维。</p>
</blockquote>
<h3 id="初始化模型参数-1"><a href="#初始化模型参数-1" class="headerlink" title="初始化模型参数"></a>初始化模型参数</h3><p>在使用net前，我们需要初始化模型参数，如线性回归模型中的权重和偏差。PyTorch在init模块中提供了多种参数初始化方法。这里的init是initializer的缩写形式。我们通过init.normal_将权重参数每个元素初始化为随机采样于均值为0、标准差为0.01的正态分布。将偏差初始化为零。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> init</span><br><span class="line">init.normal_(net[<span class="number">0</span>].weight, mean=<span class="number">0</span>, std=<span class="number">0.01</span>)</span><br><span class="line">init.constant_(net[<span class="number">0</span>].bias, val=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p>这里是直接调用了init.normal_给权重设成满足一定条件的高斯分布，init.constant_给偏差赋值。</p>
<h3 id="定义损失函数-1"><a href="#定义损失函数-1" class="headerlink" title="定义损失函数"></a>定义损失函数</h3><p>PyTorch在nn模块中提供了各种损失函数，这些损失函数可看作是一种特殊的层，PyTorch也将这些损失函数实现为nn.Module的子类。我们现在使用它提供的均方误差损失作为模型的损失函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss = nn.MSELoss()</span><br></pre></td></tr></table></figure>
<h3 id="定义优化算法-1"><a href="#定义优化算法-1" class="headerlink" title="定义优化算法"></a>定义优化算法</h3><p>同样，我们也无须自己实现小批量随机梯度下降算法。torch.optim模块提供了很多常用的优化算法比如SGD、Adam和RMSProp等。下面我们创建一个用于优化net所有参数的优化器实例，并指定学习率为0.03的小批量随机梯度下降（SGD）为优化算法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.03</span>)</span><br><span class="line">print(optimizer)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">SGD (</span><br><span class="line">Parameter Group <span class="number">0</span></span><br><span class="line">    dampening: <span class="number">0</span></span><br><span class="line">    lr: <span class="number">0.03</span></span><br><span class="line">    momentum: <span class="number">0</span></span><br><span class="line">    nesterov: <span class="literal">False</span></span><br><span class="line">    weight_decay: <span class="number">0</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>我们还可以为不同子网络设置不同的学习率，这在finetune时经常用到。例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">optimizer =optim.SGD([</span><br><span class="line">                <span class="comment"># 如果对某个参数不指定学习率，就使用最外层的默认学习率</span></span><br><span class="line">                &#123;<span class="string">'params'</span>: net.subnet1.parameters()&#125;, <span class="comment"># lr=0.03</span></span><br><span class="line">                &#123;<span class="string">'params'</span>: net.subnet2.parameters(), <span class="string">'lr'</span>: <span class="number">0.01</span>&#125;</span><br><span class="line">            ], lr=<span class="number">0.03</span>)</span><br></pre></td></tr></table></figure>
<br>
有时候我们不想让学习率固定成一个常数，那**如何调整学习率呢**？主要有两种做法。一种是修改optimizer.param_groups中对应的学习率，另一种是更简单也是较为推荐的做法——新建优化器，由于optimizer十分轻量级，构建开销很小，故而可以构建新的optimizer。但是后者对于使用动量的优化器（如Adam），会丢失动量等状态信息，可能会造成损失函数的收敛出现震荡等情况。

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 调整学习率</span></span><br><span class="line"><span class="keyword">for</span> param_group <span class="keyword">in</span> optimizer.param_groups:</span><br><span class="line">    param_group[<span class="string">'lr'</span>] *= <span class="number">0.1</span> <span class="comment"># 学习率为之前的0.1倍</span></span><br></pre></td></tr></table></figure>
<h3 id="训练模型-1"><a href="#训练模型-1" class="headerlink" title="训练模型"></a>训练模型</h3><p>这里用到optim实例的step函数来自动迭代更新模型参数。按照小批量随机梯度下降的定义，我们在step函数中指明批量大小，从而对批量中样本梯度求平均。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">num_epochs = <span class="number">3</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">1</span>, num_epochs+<span class="number">1</span>):</span><br><span class="line">    <span class="keyword">for</span> X,y <span class="keyword">in</span> data_iter:</span><br><span class="line">        output = net(X)</span><br><span class="line">        l = loss(output, y.view(<span class="number">-1</span>,<span class="number">1</span>))</span><br><span class="line">        optimizer.zero_grad() <span class="comment">#梯度清零</span></span><br><span class="line">        l.backward()</span><br><span class="line">        optimizer.step() <span class="comment">#更新参数</span></span><br><span class="line">    print(<span class="string">'epoch %d, loss: %f'</span> % (epoch, l.item()))</span><br></pre></td></tr></table></figure>
<h3 id="线性回归小结"><a href="#线性回归小结" class="headerlink" title="线性回归小结"></a>线性回归小结</h3><p>torch.utils.data模块提供了有关数据处理的工具，torch.nn模块定义了大量神经网络的层，torch.nn.init模块定义了各种初始化方法，torch.optim模块提供了很多常用的优化算法。<br><Br><br><br></p>
<h2 id="softmax回归"><a href="#softmax回归" class="headerlink" title="softmax回归"></a>softmax回归</h2><p>回顾线性回归模型，更加适用于输出为<strong>连续值</strong>的情景。而如果是分类问题，则输出为<strong>离散值</strong>如1，2，3，此时连续值到离散值的转化通常会影响到分类质量。而softmax运算使输出更适合离散值的预测。</p>
<p>softmax回归跟线性回归一样将输入特征与权重做线性叠加。与线性回归的一个主要不同在于，<strong>softmax回归的输出值个数等于标签里的类别数</strong>。因为一共有4种特征和3种输出动物类别，所以权重包含12个标量（带下标的w）、偏差包含3个标量（带下标的b），且对每个输入计算出o1,o2,o3这3个输出：<br><img src="https://imgvideoforblog.oss-cn-shanghai.aliyuncs.com/20200204%E5%9B%BE2.png" alt="Alt"><br>既然分类问题需要得到离散的预测输出，一个简单的办法是<strong>将输出值oi当作预测类别是i的置信度，并将值最大的输出所对应的类作为预测输出</strong>，即输出argmaxoi。</p>
<p>对于置信度的计算，我们便用上<strong>softmax运算</strong>，即取指数并且算概率，是一种概率计算方法，这样的话便将输出值变换成<strong>值为正且和为1</strong>的概率分布：<br><img src="https://imgvideoforblog.oss-cn-shanghai.aliyuncs.com/20200204%E5%9B%BE3.png" alt="Alt"></p>
<h3 id="单样本分类的矢量计算表达式"><a href="#单样本分类的矢量计算表达式" class="headerlink" title="单样本分类的矢量计算表达式"></a>单样本分类的矢量计算表达式</h3><p><img src="https://imgvideoforblog.oss-cn-shanghai.aliyuncs.com/20200204%E5%9B%BE4.png" alt="Alt"></p>
<h3 id="小批量样本分类的矢量计算表达式"><a href="#小批量样本分类的矢量计算表达式" class="headerlink" title="小批量样本分类的矢量计算表达式"></a>小批量样本分类的矢量计算表达式</h3><p><img src="https://imgvideoforblog.oss-cn-shanghai.aliyuncs.com/20200204%E5%9B%BE5.png" alt="Alt"><br>b广播机制从(1,q)维度到(n,q)维度。<br>理解XW+b矩阵(n,q)的含义：元素aij表示第i个样本属于第q类的值。<br>然后，很重要，记得要用softmax运算处理结果，即该样本属于各个类的概率。</p>
<h3 id="交叉熵损失函数"><a href="#交叉熵损失函数" class="headerlink" title="交叉熵损失函数"></a>交叉熵损失函数</h3><p>这里引出一个疑问：既然softmax也是线性运算只是加了个softmax运算，<strong>为何损失函数不用最小二乘法误差呢？</strong><br>其实，想要预测分类结果正确，我们其实并不需要预测概率完全等于标签概率。例如，一个真正属于类2的样本，只需要其属于类2的概率大于其他几类即可，而不强求你属于类2的概率具体是多少，然而这便是最小二乘法算误差会考虑进去的，没必要，过于严格。</p>
<p>改善上述问题的一个方法是使用更适合衡量两个概率分布差异的测量函数。其中，<strong>交叉熵（cross entropy）</strong>是一个常用的衡量方法：<br><img src="https://imgvideoforblog.oss-cn-shanghai.aliyuncs.com/20200204%E5%9B%BE6.png" alt="alt"></p>
<p>也就是说，先对每个训练样本生成一个<strong>One hot形式</strong>，如它是属于第二个类的，则该one hot为（0，1，0，0）。<strong>而交叉熵只关心对正确类别的预测概率，因为只要其值足够大，就可以确保分类结果正确</strong>。所以将1或0与该类的置信度相乘，最终只得到真正标签的置信度大小，我们目标函数希望它大，注意前面加个负号，即希望损失函数最小化。</p>
<Br>

<h3 id="softmax回归概念小结"><a href="#softmax回归概念小结" class="headerlink" title="softmax回归概念小结"></a>softmax回归概念小结</h3><p>之前反反复复看了很多遍softmax回归，发现这里讲的是最明白的，胜过西瓜书所讲，让我印象最深。<br>此处小结我重新加深认识的几个问题：<br><strong>1）softmax回归和线性回归的关系和区别</strong>：<br>共同点：在于运算都是线性函数y=wx+b。<br>区别：①softmax回归用于分类，是离散值的预测，类似于将每个类设为一个离散值，所以输出的是离散值。而线性回归用于连续值的预测。<br>②softmax在计算概率时除了线性函数的计算，还运用了softmax计算，将结果变为正且和为1的概率值。<br>③softmax损失函数用交叉熵，线性回归用最小二乘法误差计算。</p>
<p>2）我对<strong>softmax回归的计算细节</strong>印象更深刻，详见模型介绍。其计算的结果是每个样本对每个类的置信度。</p>
<p>3）<strong>为什么softmax要用交叉熵损失函数而非最小二乘法误差</strong>，因为我需要的是正确类别的置信度最大，而非精确到某个值。</p>
<p>4）<strong>交叉熵损失函数的理解</strong>。<br>以前我都是死记硬背这个函数，导致隔段时间就会记忆模糊，这里把我讲懂之后相信会记得长久且精确。<br>这里对交叉熵的理解在于，结合softmax回归的目标：让样本计算出来的属于正确标记的概率最大，所以其实只需要关注属于正确标记的概率。于是用one hot形式，不是正确标记的话yj便=0，相当于忽略该错误标记的概率，而只看正确标记即yj=1时对应的概率。</p>
<h2 id="softmax回归实现"><a href="#softmax回归实现" class="headerlink" title="softmax回归实现"></a>softmax回归实现</h2><h3 id="数据集和工具包"><a href="#数据集和工具包" class="headerlink" title="数据集和工具包"></a>数据集和工具包</h3><p>Fashion-MNIST数据集。<br>本节我们将使用torchvision包，它是服务于PyTorch深度学习框架的，主要用来构建计算机视觉模型。torchvision主要由以下几部分构成：<br>torchvision.datasets: 一些加载数据的函数及常用的数据集接口；<br>torchvision.models: 包含常用的模型结构（含预训练模型），例如AlexNet、VGG、ResNet等；<br>torchvision.transforms: 常用的图片变换，例如裁剪、旋转等；<br>torchvision.utils: 其他的一些有用的方法。</p>
<p><strong>导入工具包</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">".."</span>) <span class="comment">#为了导入上层目录的d2lzh_pytorch</span></span><br><span class="line"><span class="keyword">import</span> d2lzh_pytorch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure>

<p><strong>获取数据集</strong>：<br>我们通过torchvision的<strong>torchvision.datasets</strong>来下载这个数据集。第一次调用时会自动从网上获取数据。我们通过参数train来指定获取训练数据集或测试数据集（testing data set）。测试数据集也叫测试集（testing set），只用来评价模型的表现，并不用来训练模型。<br>另外我们还指定了参数<strong>transform = transforms.ToTensor()</strong>使所有数据转换为Tensor，如果不进行转换则返回的是PIL图片。transforms.ToTensor()将尺寸为 (H x W x C) 且数据位于[0, 255]的PIL图片或者数据类型为np.uint8的NumPy数组转换为尺寸为(C x H x W)且数据类型为torch.float32且位于[0.0, 1.0]的Tensor。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mnist_train = torchvision.datasets.FashionMNIST(root=<span class="string">'~/Datasets/FashionMNIST'</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=transforms.ToTensor())</span><br><span class="line">mnist_test = torchvision.datasets.FashionMNIST(root=<span class="string">'~/Datasets/FashionMNIST'</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>, transform=transforms.ToTensor())</span><br></pre></td></tr></table></figure>
<p>Fashion-MNIST中一共包括了10个类别，分别为t-shirt（T恤）、trouser（裤子）、pullover（套衫）、dress（连衣裙）、coat（外套）、sandal（凉鞋）、shirt（衬衫）、sneaker（运动鞋）、bag（包）和ankle boot（短靴）。</p>
<p>查看数据特征的规模：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">feature, label = mnist_train[<span class="number">0</span>]</span><br><span class="line">print(feature.shape, label)  <span class="comment"># Channel x Height x Width</span></span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([<span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>]) tensor(<span class="number">9</span>)</span><br></pre></td></tr></table></figure>
<p>由以上输出可见，feature对应高和宽均为28像素的图像。<br>需要注意的是，feature的尺寸是 (C x H x W) 的，而不是 (H x W x C)。第一维是通道数，因为数据集中是灰度图像，所以通道数为1。后面两维分别是图像的高和宽。</p>
<p>以下函数可以将数值标签转成相应的文本标签。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_fashion_mnist_labels</span><span class="params">(labels)</span>:</span></span><br><span class="line">    text_labels = [<span class="string">'t-shirt'</span>, <span class="string">'trouser'</span>, <span class="string">'pullover'</span>, <span class="string">'dress'</span>, <span class="string">'coat'</span>,</span><br><span class="line">                   <span class="string">'sandal'</span>, <span class="string">'shirt'</span>, <span class="string">'sneaker'</span>, <span class="string">'bag'</span>, <span class="string">'ankle boot'</span>]</span><br><span class="line">    <span class="keyword">return</span> [text_labels[int(i)] <span class="keyword">for</span> i <span class="keyword">in</span> labels]<span class="comment">#返回labels数组中类别序号对应的文本标签</span></span><br></pre></td></tr></table></figure>

<p>接下来<strong>画图展示训练数据</strong>，定义一个可以在一行里画出多张图像和对应标签的函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_fashion_mnist</span><span class="params">(images, labels)</span>:</span></span><br><span class="line">    _, figs = plt.subplots(<span class="number">1</span>, len(images), figsize=(<span class="number">12</span>,<span class="number">12</span>)) <span class="comment">#规模是一行有len(images)个图，每个图片12*12。这里的_表示我们忽略（不使用）的变量。</span></span><br><span class="line">    <span class="keyword">for</span> f,img,lbl <span class="keyword">in</span> zip(figs, images, labels):</span><br><span class="line">        <span class="comment">#f是图，img是要展现的数据，labels是对应的文本标签</span></span><br><span class="line">        f.imshow(img.view((<span class="number">28</span>,<span class="number">28</span>)).numpy())</span><br><span class="line">        <span class="comment">#注意数据是tensor，要转换成numpy数组再传入</span></span><br><span class="line">        f.set_title(lbl)</span><br><span class="line">        f.axes.get_xaxis().set_visible(<span class="literal">False</span>)</span><br><span class="line">        f.axes.get_yacis().set_visible(<span class="literal">False</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">X, y = [], []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    X.append(mnist_train[i][<span class="number">0</span>])</span><br><span class="line">    y.append(mnist_train[i][<span class="number">1</span>])</span><br><span class="line">show_fashion_mnist(X, get_fashion_minist_labels(y)) <span class="comment">#这里将标签序号转成文本标签</span></span><br></pre></td></tr></table></figure>

<p><strong>读取小批量</strong>：<br>我们将在训练数据集上训练模型，并将训练好的模型在测试数据集上评价模型的表现。前面说过，mnist_train是torch.utils.data.Dataset的子类，所以我们可以将其传入<strong>torch.utils.data.DataLoader</strong>来创建一个读取小批量数据样本的DataLoader实例。</p>
<p>在实践中，数据读取经常是训练的性能瓶颈，特别当模型较简单或者计算硬件性能较高时。PyTorch的DataLoader中一个很方便的功能是<strong>允许使用多进程来加速数据读取</strong>。这里我们通过参数num_workers来设置4个进程读取数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">256</span></span><br><span class="line"><span class="keyword">if</span> sys.platform.startswith(<span class="string">'win'</span>):</span><br><span class="line">    num_workers = <span class="number">0</span> <span class="comment"># 0表示不用额外的进程来加速读取数据</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    num_workders = <span class="number">4</span></span><br><span class="line">train_iter = torch.utils.data.DataLoader(mnist_train, batch_size = batch_size, shuffle=<span class="literal">True</span>, num_workers = num_workers)</span><br><span class="line">test_iter = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, shuffle=<span class="literal">False</span>, num_workers=num_workers)</span><br></pre></td></tr></table></figure>
<p>这里加深印象：dataloader传入数据集、batch_size、是否要shuffle、(多进程加速读取数据的进程数)。然后便可以从train_iter里循环取数据了。</p>
<br>

<h3 id="获取和读取数据"><a href="#获取和读取数据" class="headerlink" title="获取和读取数据"></a>获取和读取数据</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br></pre></td></tr></table></figure>
<Br>

<h3 id="初始化模型参数-2"><a href="#初始化模型参数-2" class="headerlink" title="初始化模型参数"></a>初始化模型参数</h3><p>已知每个样本输入是高和宽均为28像素的图像，所以模型的输入向量长度是28<em>28=784，所以softmax回归的权重和偏差参数分别为784</em>10和1*10的矩阵。<br>W用高斯分布初始化，b用0初始化，主要注意规模设置。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">num_inputs = <span class="number">784</span></span><br><span class="line">num_outputs = <span class="number">10</span></span><br><span class="line">W = torch.tensor(np.random.normal(<span class="number">0</span>,<span class="number">0.01</span>, (num_inputs, num_outputs)), dtype=torch.float)</span><br><span class="line">b = torch.zeros(num_outputs, dtype=torch,float)</span><br></pre></td></tr></table></figure>
<p>注意，初始化的同时<strong>设置好参数的梯度</strong>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">W.requires_grad_(<span class="literal">True</span>)</span><br><span class="line">b.requires_grad_(<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<br>

<h3 id="实现softmax运算"><a href="#实现softmax运算" class="headerlink" title="实现softmax运算"></a>实现softmax运算</h3><p>首先要理解如何对多维Tensor<strong>按维度操作</strong>。<br>以sum函数为例，其实就是对参数dim设置，我们可以只对同一列（dim=0）或同一行（dim=1）的元素求和，并且在结果中保留行和列这两个维度（keepdim=True)。</p>
<p>示例代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">X = torch.tensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],</span><br><span class="line">                  [<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])</span><br><span class="line">print(X.sum(dim=<span class="number">0</span>, keepdim=<span class="literal">True</span>)) <span class="comment">#dim=0是按列</span></span><br><span class="line">print(X.sum(dim=<span class="number">1</span>, keepdim=<span class="literal">True</span>)) <span class="comment">#dim=1是按行</span></span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensor([[<span class="number">5</span>, <span class="number">7</span>, <span class="number">9</span>]])</span><br><span class="line">tensor([[ <span class="number">6</span>],</span><br><span class="line">        [<span class="number">15</span>]])</span><br></pre></td></tr></table></figure>
<Br>
所以在这里如何实现softmax运算呢？根据之前的定义，softmax运算会先通过exp函数对每个元素做指数运算，再对exp矩阵同行元素求和，最后令矩阵每行各元素与该行元素之和相除。

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sotmax</span><span class="params">(X)</span>:</span></span><br><span class="line">    X_exp = X.exp()</span><br><span class="line">    partition = X_exp.sum(dim=<span class="number">1</span>, keepdim=<span class="literal">True</span>) <span class="comment">#按行求和！</span></span><br><span class="line">    <span class="keyword">return</span> X_exp/partition <span class="comment">#广播机制！</span></span><br></pre></td></tr></table></figure>
<br>

<h3 id="定义模型-2"><a href="#定义模型-2" class="headerlink" title="定义模型"></a>定义模型</h3><p>softmax回归就是简单的线性函数+softmax运算，这里简单的线性函数<strong>注意矩阵相乘的维度，需要用view函数来reshape</strong>，把X矩阵规模变成（样本数，784）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">net</span><span class="params">(X)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> softmax( torch.mm(X.view((<span class="number">-1</span>,num_inputs)), W) + b)</span><br></pre></td></tr></table></figure>
<p>这里千万注意维度要用()括起来，所以view那里要两个括号。<br><br></p>
<h3 id="定义损失函数-2"><a href="#定义损失函数-2" class="headerlink" title="定义损失函数"></a>定义损失函数</h3><p>我们介绍了softmax回归使用的交叉熵损失函数。为了得到标签的预测概率，我们可以使用<strong>gather函数</strong>。</p>
<blockquote>
<p>gather函数：Gathers values along an axis specified by dim.</p>
</blockquote>
<p>即给定<em>维度</em>（要以行为单位还是列为单位）和<em>索引</em>，从一个tensor中提取索引对应的元素作为结果。<br>注意y作为索引，还是要用view来reshape成单列的，好与行作对应。<br>示例代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">y_hat = torch.tensor([[<span class="number">0.1</span>, <span class="number">0.3</span>, <span class="number">0.6</span>], [<span class="number">0.3</span>, <span class="number">0.2</span>, <span class="number">0.5</span>]])</span><br><span class="line">y = torch.LongTensor([<span class="number">0</span>, <span class="number">2</span>])</span><br><span class="line">y_hat.gather(<span class="number">1</span>, y.view(<span class="number">-1</span>, <span class="number">1</span>)) <span class="comment">#dim=1即按行，取第一行的第0个，取第二行的第2个</span></span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([[<span class="number">0.1000</span>],</span><br><span class="line">        [<span class="number">0.5000</span>]])</span><br></pre></td></tr></table></figure>
<br>

<p>下面实现<strong>交叉熵损失函数</strong>：<br>根据交叉熵的定义，我们只需要用gather函数取出预测结果y_hat里的属于真实标签对应的概率再取对数。<br><del>不需要one hot和求和操作了。</del> </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cross_entropy</span><span class="params">(y_hat, y)</span>:</span> <span class="comment">#y_hat是模型预测出来的，每行是该样本对各类对应的预测概率，y是该样本的真实标签序号</span></span><br><span class="line">    <span class="keyword">return</span> - torch.log(y_hat.gather(<span class="number">1</span>, y.view((<span class="number">-1</span>,<span class="number">1</span>)))</span><br></pre></td></tr></table></figure>
<Br>

<h3 id="计算分类准确率"><a href="#计算分类准确率" class="headerlink" title="计算分类准确率"></a>计算分类准确率</h3><p>给定一个类别的预测概率分布y_hat，我们把预测概率最大的类别作为输出类别。如果它与真实类别y一致，说明这次预测是正确的。分类准确率即正确预测数量与总预测数量之比。</p>
<p>为了演示准确率的计算，下面定义准确率accuracy函数。其中y_hat.argmax(dim=1)返回矩阵y_hat每行中最大元素的索引，且返回结果与变量y形状相同(这里说一下，这里训练数据的标签y数组本就是1列，不同于上方示例定义的y规模）。<br>相等条件判断式(y_hat.argmax(dim=1) == y)是一个类型为<strong>ByteTensor的Tensor</strong>，我们用float()将其转换为值为0（相等为假）或1（相等为真）的浮点型Tensor。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">accuracy</span><span class="params">(y_hat, y)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (y_hat.argmax(dim=<span class="number">1</span>) ==y).float().mean().item()</span><br></pre></td></tr></table></figure>
<p>类似地，我们可以评价模型net在数据集data_iter上的准确率。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate_accuracy</span><span class="params">(data_iter, net)</span>:</span></span><br><span class="line">    acc_sum, n=<span class="number">0.0</span>,<span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> X,y <span class="keyword">in</span> data_iter:</span><br><span class="line">        <span class="comment">#先累加准确率</span></span><br><span class="line">        acc_sum += (net(X).argmax(dim=<span class="number">1</span>) ==y).float().sum().item()</span><br><span class="line">        n += y.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">return</span> acc_sum/n</span><br></pre></td></tr></table></figure>
<br>

<h3 id="训练模型-2"><a href="#训练模型-2" class="headerlink" title="训练模型"></a>训练模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">num_epochs, lr = <span class="number">5</span>, <span class="number">0.1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 本函数已保存在d2lzh包中方便以后使用</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_ch3</span><span class="params">(net, train_iter, test_iter, loss, num_epochs, batch_size,</span></span></span><br><span class="line"><span class="function"><span class="params">              params=None, lr=None, optimizer=None)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        train_l_sum, train_acc_sum, n = <span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">            y_hat = net(X)</span><br><span class="line">            l = loss(y_hat, y).sum()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 梯度清零</span></span><br><span class="line">            <span class="keyword">if</span> optimizer <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>: <span class="comment">#简化实现</span></span><br><span class="line">                optimizer.zero_grad()</span><br><span class="line">            <span class="keyword">elif</span> params <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> params[<span class="number">0</span>].grad <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">                    param.grad.data.zero_()</span><br><span class="line"></span><br><span class="line">            l.backward()</span><br><span class="line">            <span class="keyword">if</span> optimizer <span class="keyword">is</span> <span class="literal">None</span>: </span><br><span class="line">                d2l.sgd(params, lr, batch_size)</span><br><span class="line">            <span class="keyword">else</span>: <span class="comment">#简化实现</span></span><br><span class="line">                optimizer.step()  <span class="comment"># “softmax回归的简洁实现”一节将用到</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            train_l_sum += l.item()</span><br><span class="line">            train_acc_sum += (y_hat.argmax(dim=<span class="number">1</span>) == y).sum().item()</span><br><span class="line">            n += y.shape[<span class="number">0</span>]</span><br><span class="line">        test_acc = evaluate_accuracy(test_iter, net)</span><br><span class="line">        print(<span class="string">'epoch %d, loss %.4f, train acc %.3f, test acc %.3f'</span></span><br><span class="line">              % (epoch + <span class="number">1</span>, train_l_sum / n, train_acc_sum / n, test_acc))</span><br><span class="line"></span><br><span class="line">train_ch3(net, train_iter, test_iter, cross_entropy, num_epochs, batch_size, [W, b], lr)</span><br></pre></td></tr></table></figure>
<h3 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h3><p>训练完成后，现在就可以演示如何对图像进行分类了。给定一系列图像（第三行图像输出），我们比较一下它们的真实标签（第一行文本输出）和模型预测结果（第二行文本输出）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">X, y = iter(test_iter).next()</span><br><span class="line"></span><br><span class="line">true_labels = d2l.get_fashion_mnist_labels(y.numpy())</span><br><span class="line">pred_labels = d2l.get_fashion_mnist_labels(net(X).argmax(dim=<span class="number">1</span>).numpy())</span><br><span class="line">titles = [true + <span class="string">'\n'</span> + pred <span class="keyword">for</span> true, pred <span class="keyword">in</span> zip(true_labels, pred_labels)]</span><br><span class="line"></span><br><span class="line">d2l.show_fashion_mnist(X[<span class="number">0</span>:<span class="number">9</span>], titles[<span class="number">0</span>:<span class="number">9</span>])</span><br></pre></td></tr></table></figure>
<p><a href="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWd2aWRlb2ZvcmJsb2cub3NzLWNuLXNoYW5naGFpLmFsaXl1bmNzLmNvbS8yMDIwMDIwNCVFNSU5QiVCRTcucG5n?x-oss-process=image/format,png" target="_blank" rel="noopener">https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWd2aWRlb2ZvcmJsb2cub3NzLWNuLXNoYW5naGFpLmFsaXl1bmNzLmNvbS8yMDIwMDIwNCVFNSU5QiVCRTcucG5n?x-oss-process=image/format,png</a>)<br><Br></p>
<h2 id="softmax回归简洁实现"><a href="#softmax回归简洁实现" class="headerlink" title="softmax回归简洁实现"></a>softmax回归简洁实现</h2><h3 id="获取和读取数据-1"><a href="#获取和读取数据-1" class="headerlink" title="获取和读取数据"></a>获取和读取数据</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br></pre></td></tr></table></figure>
<p><img src="https://imgvideoforblog.oss-cn-shanghai.aliyuncs.com/20200204%E5%9B%BE7.png" alt="alt"><br><Br></p>
<h3 id="定义和初始化模型"><a href="#定义和初始化模型" class="headerlink" title="定义和初始化模型"></a>定义和初始化模型</h3><p>softmax回归的输出层是一个全连接层，所以我们用一个线性模块就可以了。因为前面我们数据返回的每个batch样本x的形状为(batch_size, 1, 28, 28), 所以我们要先用view()将x的形状转换成(batch_size, 784)才送入全连接层。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">num_inputs = <span class="number">784</span></span><br><span class="line">num_outputs = <span class="number">10</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LinearNet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__int__</span><span class="params">(self, num_inputs, num_outputs)</span>:</span></span><br><span class="line">        super(LinearNet, self).__init__()</span><br><span class="line">        self.linear = nn.Linear(num_inputs, num_outputs)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span> <span class="comment">#x shape:(batch, 1, 28, 28) 设置前向传播 即这一层的操作是啥</span></span><br><span class="line">        y = self.linear(x.view(x.shape[<span class="number">0</span>], <span class="number">-1</span>)) <span class="comment">#即将输入数据的规模变成（batch,784）</span></span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line">net = LinearNet(num_inputs, num_outputs)</span><br></pre></td></tr></table></figure>
<p>但是呢，其实我们可以将对x的形状转换的这个功能自定义一个FlattenLayer，并记录在d2lzh_pytorch中方便后面使用。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 本函数已保存在d2lzh_pytorch包中方便以后使用</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FlattenLayer</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(FlattenLayer, self).__init__()</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span> <span class="comment"># x shape: (batch, *, *, ...)</span></span><br><span class="line">        <span class="keyword">return</span> x.view(x.shape[<span class="number">0</span>], <span class="number">-1</span>)</span><br></pre></td></tr></table></figure>
<p><em>我现在对这种class类的定义有了些许理解，即<strong>init</strong>相当于构造函数，传入的第一个参数必须是self，当然也可以去初始化类中的属性；对于forward函数则类似于阐述该类的主要操作是什么。</em></p>
<br>
由此，我们**定义模型**（两层，一层为将样本规模拉成样本数*784，第二层即线性函数层），并且使用OrderedDict来给网络的各层命名。
注意，由于有第一层转换规模了，所以第二层直接用工具包自带的nn.Linear，而非我们刚刚定义的LinearNet层。

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> coleections <span class="keyword">import</span> OrderedDict</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(</span><br><span class="line">        OrderedDict([</span><br><span class="line">            (<span class="string">'flatten'</span>, FlattenLayer()),</span><br><span class="line">            (<span class="string">'linear'</span>, nn.Linear(num_inputs, num_outputs))</span><br><span class="line">                    ])</span><br><span class="line">                    )</span><br></pre></td></tr></table></figure>
<p>然后，初始化参数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> init</span><br><span class="line"></span><br><span class="line">init.normal_(net.linear.weight, mean=<span class="number">0</span>, std=<span class="number">0.01</span>)</span><br><span class="line">init.constant(net.linear.bias, val=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<br>

<h3 id="softmax和交叉熵损失函数"><a href="#softmax和交叉熵损失函数" class="headerlink" title="softmax和交叉熵损失函数"></a>softmax和交叉熵损失函数</h3><p>PyTorch提供了一个包括softmax运算和交叉熵损失计算的函数。它的数值稳定性更好。<br>相当于<em>在定义模型的时候也不用考虑softmax运算了，在计算损失函数的时候会一起算好</em>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss = nn.CrossEntropyLoss()</span><br></pre></td></tr></table></figure>

<br>

<h3 id="定义优化算法-2"><a href="#定义优化算法-2" class="headerlink" title="定义优化算法"></a>定义优化算法</h3><p>我们使用学习率为0.1的小批量随机梯度下降作为优化算法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure>
<br>

<h3 id="训练模型-3"><a href="#训练模型-3" class="headerlink" title="训练模型"></a>训练模型</h3><p>接下来，我们使用上一节中定义的训练函数来训练模型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">num_epochs = <span class="number">5</span></span><br><span class="line">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, <span class="literal">None</span>, <span class="literal">None</span>, optimizer)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">epoch <span class="number">1</span>, loss <span class="number">0.0031</span>, train acc <span class="number">0.745</span>, test acc <span class="number">0.790</span></span><br><span class="line">epoch <span class="number">2</span>, loss <span class="number">0.0022</span>, train acc <span class="number">0.812</span>, test acc <span class="number">0.807</span></span><br><span class="line">epoch <span class="number">3</span>, loss <span class="number">0.0021</span>, train acc <span class="number">0.825</span>, test acc <span class="number">0.806</span></span><br><span class="line">epoch <span class="number">4</span>, loss <span class="number">0.0020</span>, train acc <span class="number">0.832</span>, test acc <span class="number">0.810</span></span><br><span class="line">epoch <span class="number">5</span>, loss <span class="number">0.0019</span>, train acc <span class="number">0.838</span>, test acc <span class="number">0.823</span></span><br></pre></td></tr></table></figure>
<br>

<h3 id="softmax回归小结"><a href="#softmax回归小结" class="headerlink" title="softmax回归小结"></a>softmax回归小结</h3><p>在这里简单对比下softmax回归的从零实现和简洁实现。<br>其实最大的不同就是，简洁实现用上了torch.nn工具包里的封装好的函数，体现在：</p>
<p>① 定义网络模型：从零实现都是用“函数def”来定义实现功能，而简洁实现为了用nn.Sequential来构建多层神经网络，需要事先对每层网络用“类class”来定义。<br>必须要注意的是，由于损失函数采用nn.CrossEntropyLoss()，所以也不用在定义网络模型的时候手动添加softmax运算了，会在算损失函数的时候一并自动算好，而且也不用再额外手动用gather函数定义交叉熵损失函数了，更方便了。</p>
<p>②训练模型：由于引入了optimizer(这里是torch.optim.SGD)，训练中直接用optimizer.zero_grad来清零参数的梯度，直接用optimizer.step()来进行反向传播之后的梯度下降迭代更新。<br>从而，简便了对各参数进行grad_zero_和sgd（即减去(学习率lr*参数当前梯度)/batch_size进行更新的操作）。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/02/04/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E5%92%8Csoftmax%E5%9B%9E%E5%BD%92/" data-id="ck67m9c240000a86chjjhasvx"
         class="article-share-link">Share</a>
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag">动手学深度学习</a></li></ul>

    </footer>

  </div>

  
    
  <nav class="article-nav">
    
    
      <a href="/2020/02/02/pytorch%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86/" class="article-nav-link">
        <strong class="article-nav-caption">Olde posts</strong>
        <div class="article-nav-title">pytorch预备知识</div>
      </a>
    
  </nav>


  

  
    
  <div class="gitalk" id="gitalk-container"></div>
  
<link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">

  
<script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>

  
<script src="https://cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.min.js"></script>

  <script type="text/javascript">
    var gitalk = new Gitalk({
      clientID: 'dd9dd0a01eb6aea2b4b2',
      clientSecret: 'f10df8d5a64e14405c16d484ff17f526f1ff2c21',
      repo: 'gitalk_blog',
      owner: 'JosephLZD',
      admin: ['JosephLZD'],
      // id: location.pathname,      // Ensure uniqueness and length less than 50
      id: md5(location.pathname),
      distractionFreeMode: false,  // Facebook-like distraction free mode
      pagerDirection: 'last'
    })

  gitalk.render('gitalk-container')
  </script>

  

</article>



</section>
  <footer class="footer">
  <div class="outer">
    <div class="float-right">
      <ul class="list-inline">
  
    <li><i class="fe fe-smile-alt"></i> <span id="busuanzi_value_site_uv"></span></li>
  
</ul>
    </div>
    <ul class="list-inline">
      <li>&copy; 2020 Blog_JosephLZD</li>
      <li>Powered by <a href="http://hexo.io/" target="_blank">Hexo</a></li>
    </ul>
  </div>
</footer>

</main>

<aside class="sidebar sidebar-specter">
  
    <button class="navbar-toggle"></button>
<nav class="navbar">
  
    <div class="logo">
      <a href="/"><img src="/images/hexo.svg" alt="Blog_JosephLZD"></a>
    </div>
  
  <ul class="nav nav-main">
    
      <li class="nav-item">
        <a class="nav-item-link" href="/">Home</a>
      </li>
    
      <li class="nav-item">
        <a class="nav-item-link" href="/archives">Archive</a>
      </li>
    
      <li class="nav-item">
        <a class="nav-item-link" href="/gallery">Gallery</a>
      </li>
    
      <li class="nav-item">
        <a class="nav-item-link" href="/about">About</a>
      </li>
    
    <li class="nav-item">
      <a class="nav-item-link nav-item-search" title="搜索">
        <i class="fe fe-search"></i>
        Search
      </a>
    </li>
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      <div class="totop" id="totop">
  <i class="fe fe-rocket"></i>
</div>
    </li>
    <li class="nav-item">
      
        <a class="nav-item-link" target="_blank" href="/atom.xml" title="RSS Feed">
          <i class="fe fe-feed"></i>
        </a>
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
  </aside>
  
<script src="/js/jquery-2.0.3.min.js"></script>


<script src="/js/jquery.justifiedGallery.min.js"></script>


<script src="/js/lazyload.min.js"></script>


<script src="/js/busuanzi-2.3.pure.min.js"></script>


  
<script src="/fancybox/jquery.fancybox.min.js"></script>




  
<script src="/js/tocbot.min.js"></script>

  <script>
    // Tocbot_v4.7.0  http://tscanlin.github.io/tocbot/
    tocbot.init({
      tocSelector: '.tocbot',
      contentSelector: '.article-entry',
      headingSelector: 'h1, h2, h3, h4, h5, h6',
      hasInnerContainers: true,
      scrollSmooth: true,
      positionFixedSelector: '.tocbot',
      positionFixedClass: 'is-position-fixed',
      fixedSidebarOffset: 'auto',
    });
  </script>



<script src="/js/ocean.js"></script>


</body>
</html>