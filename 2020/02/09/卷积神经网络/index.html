<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  
  
  
    <meta name="description" content="Ain&#39;t no mountain high enough">
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <title>
    卷积神经网络 |
    
    Blog_JosephLZD</title>
  
    <link rel="shortcut icon" href="/favicon.ico">
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<script src="/js/pace.min.js"></script>

<meta name="generator" content="Hexo 4.1.1"></head>

<body>
<main class="content">
  <section class="outer">
  

<article id="post-卷积神经网络" class="article article-type-post" itemscope itemprop="blogPost" data-scroll-reveal>
  
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      卷积神经网络
    </h1>
  
  




      </header>
    

    
      <div class="article-meta">
        <a href="/2020/02/09/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" class="article-date">
  <time datetime="2020-02-09T10:28:59.000Z" itemprop="datePublished">2020-02-09</time>
</a>
        
      </div>
    

    
      
    <div class="tocbot"></div>





    

    <div class="article-entry" itemprop="articleBody">
      


      

      
        <p>《动手学深度学习pytorch版》“卷积神经网络”学习笔记。</p>
<a id="more"></a>
<h1 id="二维卷积层"><a href="#二维卷积层" class="headerlink" title="二维卷积层"></a>二维卷积层</h1><p>概念不多说了，网上都能查到。简单说下“卷积运算”，这里是<strong>指拿一个卷积核来，对应输入矩阵的同维度的部分，进行相同位置的元素对应相乘再求和的操作</strong>。</p>
<blockquote>
<p>其实这个运算方法叫“互相关运算”，深度学习中的卷积运算实际上是二维互相关运算。</p>
</blockquote>
<p>下面我们将上述过程实现在corr2d函数里。它接受输入数组X与核数组K，并输出数组Y。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">corr2d</span><span class="params">(X, K)</span>:</span>  <span class="comment"># 本函数已保存在d2lzh_pytorch包中方便以后使用</span></span><br><span class="line">    h, w = K.shape</span><br><span class="line">    Y = torch.zeros((X.shape[<span class="number">0</span>] - h + <span class="number">1</span>, X.shape[<span class="number">1</span>] - w + <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(Y.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(Y.shape[<span class="number">1</span>]):</span><br><span class="line">            Y[i, j] = (X[i: i + h, j: j + w] * K).sum() <span class="comment">#这个切片要学会，即从中取下一个矩阵！</span></span><br><span class="line">    <span class="keyword">return</span> Y</span><br></pre></td></tr></table></figure>

<p>接下来，定义网络模型类：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Conv2D</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, *args)</span>:</span></span><br><span class="line">        super(Conv2D, self).__init__()</span><br><span class="line">        self.weight = nn.Parameter(torch.randn(kernel_size)) <span class="comment">#卷积核初始化为随机吧</span></span><br><span class="line">        self.bias = nn.Parameter(torch.randn(<span class="number">1</span>)) <span class="comment">#偏置随便弄个数</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> corr2d(x, self.weight) + self.bias</span><br></pre></td></tr></table></figure>
<br>

<h2 id="学习卷积核"><a href="#学习卷积核" class="headerlink" title="学习卷积核"></a>学习卷积核</h2><p>我们来看一个例子，它使用物体边缘检测中的输入数据X和输出数据Y来学习我们构造的核数组K（学习卷积核）。我们首先构造一个卷积层，其卷积核将被初始化成随机数组。接下来在每一次迭代中，我们使用平方误差来比较Y和卷积层的输出，然后计算梯度来更新权重。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">conv2d = Conv2D(kernel_size=(<span class="number">1</span>,<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">step = <span class="number">20</span></span><br><span class="line">lr = <span class="number">0</span>,<span class="number">01</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(step):</span><br><span class="line">    Y_hat = conv2d(X)</span><br><span class="line">    l = ((Y_hat - Y) ** <span class="number">2</span>).sum()</span><br><span class="line">    l.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 梯度下降</span></span><br><span class="line">    conv2d.weight.data -= lr * conv2d.weight.grad</span><br><span class="line">    conv2d.bias.data -= lr * conv2d.bias.grad</span><br><span class="line"></span><br><span class="line">    <span class="comment">#梯度清0</span></span><br><span class="line">    conv2d.weight.grad.fill_(<span class="number">0</span>)</span><br><span class="line">    conv2d.bias.grad.fill_(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">if</span>(i+<span class="number">1</span>)%<span class="number">5</span>==<span class="number">0</span>:</span><br><span class="line">        print(<span class="string">'Step %d, loss %.3f'</span> % (i+<span class="number">1</span>, l.item()))</span><br></pre></td></tr></table></figure>

<h1 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h1><h3 id="特征图"><a href="#特征图" class="headerlink" title="特征图"></a>特征图</h3><p>二维卷积层输出的二维数组可以看作是输入在空间维度（宽和高）上某一级的表征，也叫特征图（feature map）。</p>
<h3 id="感受野"><a href="#感受野" class="headerlink" title="感受野"></a>感受野</h3><p>影响元素xx的前向计算的所有可能输入区域（可能大于输入的实际尺寸）叫做xx的感受野（receptive field）。<br>以下图为例，输入中阴影部分的四个元素是输出中阴影部分元素的感受野。<br><img src="https://imgvideoforblog.oss-cn-shanghai.aliyuncs.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202020-02-07%20%E4%B8%8B%E5%8D%884.25.10.png" alt="alt"></p>
<br>

<h1 id="填充和步幅"><a href="#填充和步幅" class="headerlink" title="填充和步幅"></a>填充和步幅</h1><h2 id="维度关系"><a href="#维度关系" class="headerlink" title="维度关系"></a>维度关系</h2><p>这里必须要强调并温习一下，关于卷积运算<strong>输入矩阵、卷积核、输出矩阵的维度之间的关系</strong>（计算公式）。<br><em>注意，以下情况特指步幅stride=1：</em><br><img src="https://imgvideoforblog.oss-cn-shanghai.aliyuncs.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202020-02-07%20%E4%B8%8B%E5%8D%884.32.54.png" alt="alt"><br><img src="https://imgvideoforblog.oss-cn-shanghai.aliyuncs.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202020-02-07%20%E4%B8%8B%E5%8D%884.33.47.png" alt="alt"><br>所以，很多情况下，我们如果想要让输入和输出矩阵具有相同的高和宽，则进行“填充“，且设置padding在行上ph=kh-1，在列上pw=kw-1。</p>
<h2 id="填充"><a href="#填充" class="headerlink" title="填充"></a>填充</h2><p>下面举个例子，想要让输入和输出的矩阵维度相同，进行填充。对于一个高和宽为3的卷积核，输入矩阵的填充分别在高和宽上增加3-1=2. 因此，给定一个高和宽为8的输入，我们发现输出的高和宽也是8。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="comment">#定义一个函数来计算卷积层</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">comp_conv2d</span><span class="params">(conv2d, X)</span>:</span></span><br><span class="line"><span class="comment"># (1, 1)代表批量大小和通道数（“多输入通道和多输出通道”一节将介绍）均为1</span></span><br><span class="line">    X = X.view((<span class="number">1</span>,<span class="number">1</span>)+X.shape)</span><br><span class="line">    Y = conv2d(X)</span><br><span class="line">    <span class="keyword">return</span> Y.view(Y.shape[<span class="number">2</span>:])<span class="comment"># 排除不关心的前两维：批量和通道</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 注意这里是两侧分别填充1行或列，所以在两侧一共填充2行或列</span></span><br><span class="line">    conv2d = nn.Conv2d(in_channels=<span class="number">1</span>, out_channels=<span class="number">1</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>) <span class="comment">#这个padding参数很重要！</span></span><br><span class="line">    </span><br><span class="line">    x=torch.rand(<span class="number">8</span>,<span class="number">8</span>)</span><br><span class="line">    comp_conv2d(conv2d, X).shape</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([<span class="number">8</span>, <span class="number">8</span>])</span><br></pre></td></tr></table></figure>
<p>注意这里<code>nn.Conv2d</code>的使用，<strong>实例化时要指定好输入矩阵和输出矩阵的channel以及卷积核的维度</strong>。也就是说，输入和输出矩阵的维度可以不事先设置好，但卷积核的维度对该卷积层来说是唯一的，所以必须事先设置好kernel_size。</p>
<p>如果要padding，就padding=1或padding=(a,b)即给行、列设置不同的填充数。<br>注意这里的padding=1参数的含义：是对高/宽的上下/左右<strong>各填充</strong>1行/列，所以其实是一共添加了2行/2列。而公式中的ph和pw是指行/列上分别一共填充了多少行/列。</p>
<blockquote>
<p>nn.Conv2d(self, in_channels, out_channels, kernel_size, stride=1,padding=0, dilation=1, groups=1, bias=True))<br>参数：<br>  in_channel:　输入数据的通道数，例RGB图片通道数为3；   out_channel: 输出数据的通道数，这个根据模型调整；<br>  kennel_size: 卷积核大小，可以是int，或tuple；kennel_size=2,意味着卷积大小2，<br>kennel_size=（2,3），意味着卷积在第一维度大小为2，在第二维度大小为3；<br>  stride：步长，默认为1，与kennel_size类似，stride=2,意味在所有维度步长为2，<br>stride=（2,3），意味着在第一维度步长为2，意味着在第二维度步长为3；   padding：　零填充</p>
</blockquote>
<p>当卷积核的高和宽不同时，我们也可以通过设置高和宽上不同的填充数使输出和输入具有相同的高和宽。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用高为5、宽为3的卷积核。在高和宽两侧的填充数分别为2和1</span></span><br><span class="line">conv2d = nn.Conv2d(in_channels=<span class="number">1</span>, out_channels=<span class="number">1</span>, kernel_size=(<span class="number">5</span>, <span class="number">3</span>), padding=(<span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line">comp_conv2d(conv2d, X).shape</span><br></pre></td></tr></table></figure>
<p>对以上代码，padding=(2,1)为什么能达到输出高宽不变？因为卷积核维度为（5，3），所以行上应该填充一共5-1=4行，所以是上下各填充2行；列上应该填充一共3-1=2行，所以是上下各填充1行。</p>
<h2 id="步幅"><a href="#步幅" class="headerlink" title="步幅"></a>步幅</h2><p><img src="https://imgvideoforblog.oss-cn-shanghai.aliyuncs.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202020-02-07%20%E4%B8%8B%E5%8D%885.01.21.png" alt="alt"><br>下面我们令高和宽上的步幅均为2，从而使输入的高和宽减半。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">conv2d = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, stride=<span class="number">2</span>) <span class="comment">#使输入的高和宽减半</span></span><br><span class="line"><span class="comment">#虽然用公式得到的是（nh+sh-1)/sh=(nh+1)/2，但是算出来是小数的话会取整仍得到nh/2。</span></span><br><span class="line">comp_conv2d(conv2d, X).shape</span><br></pre></td></tr></table></figure>

<Br>

<h1 id="多通道"><a href="#多通道" class="headerlink" title="多通道"></a>多通道</h1><p>1） <strong>多输入通道</strong><br>运算方法：对每个通道的输入矩阵进行卷积核运算，然后将这得到的多个feature map进行<strong>对应相加</strong>，得到一个通道的feature map。<br><img src="https://imgvideoforblog.oss-cn-shanghai.aliyuncs.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202020-02-07%20%E4%B8%8B%E5%8D%885.21.07.png" alt="alt"></p>
<p>代码实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">".."</span>) </span><br><span class="line"><span class="keyword">import</span> d2lzh_pytorch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">corr2d_multi_in</span><span class="params">(X, K)</span>:</span></span><br><span class="line">    <span class="comment"># 沿着X和K的第0维（通道维）分别计算再相加</span></span><br><span class="line">    res = d2l.corr2d(X[<span class="number">0</span>, :, :], K[<span class="number">0</span>, :, :]) <span class="comment">#先把第零维的结果计算出来，之后再把后面维的结果叠加</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, X.shape[<span class="number">0</span>]):</span><br><span class="line">        res += d2l.corr2d(X[i, :, :], K[i, :, :])</span><br><span class="line">    <span class="keyword">return</span> res <span class="comment">#最终的一个feature map</span></span><br></pre></td></tr></table></figure>

<p>可见，当输入通道有多个时，因为我们对各个通道的结果做了累加，所以不论输入通道数是多少，输出通道数总是为1。</p>
<p>2） <strong>多输出通道</strong><br>现在在多输入通道的基础上，加入多输出通道的条件。</p>
<p>即<strong>不仅输入矩阵有多通道，卷积核也需要要多个通道！</strong>（你想想，如果只有一个卷积核，那多输出通道的每个通道的运算结果都是一样的，没有意义，所以要卷积核不同，不同的卷积核去和不同的输入通道做运算，即feature map结果中的一个维度对应卷积核中一个维度拿去和X运算的结果）<br>所以假设，输入通道数为ci，输出通道数为co，那么卷积核应该是co*ci个，从而在每层对ci个feature map叠加，得到最终的co层输出。<br><Br></p>
<p>下面我们实现一个互相关运算函数来计算多个通道的输出。</p>
<p>首先，整个多通道的卷积核。我们将核数组K同K+1（K中每个元素加一）和K+2连结在一起来构造一个输出通道数为3的卷积核。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">K = torch.stack([K, K+<span class="number">1</span>, K+<span class="number">2</span>])</span><br><span class="line">K.shape</span><br></pre></td></tr></table></figure>

<p>然后，用之前的多输入通道时的运算函数，让每个通道的卷积核与输入矩阵的所有通道（即X）进行运算，若输入矩阵有ci层，卷积层有ai层，则得到的feature map结果有ci*ai层。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">corr2d_multi_in_out</span><span class="params">(X, K)</span>:</span></span><br><span class="line">    <span class="comment"># 对K的第0维遍历，每次同输入X做互相关计算。所有结果使用stack函数合并在一起。</span></span><br><span class="line">    <span class="keyword">return</span> torch.stack([corr2d_multi_in(X,k) <span class="keyword">for</span> k <span class="keyword">in</span> K])</span><br></pre></td></tr></table></figure>
<br>

<h1 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h1><p>之前讲到卷积运算对识别图像边缘的效果，但是，实际图像里，我们感兴趣的物体不会总出现在固定位置：<em>即使我们连续拍摄同一个物体也极有可能出现像素位置上的偏移</em>。这会导致同一个边缘对应的输出可能出现在卷积输出Y中的不同位置，进而对后面的模式识别造成不便。</p>
<p>在本节中我们介绍池化（pooling）层，它的提出是为了<strong>缓解卷积层对位置的过度敏感性</strong>。</p>
<p>池化层的运算在形式上很类似于卷积层，也就是取一个一般小于输入规模的窗口，在输入上”滑动“，直接计算池化窗口内元素的最大值或者平均值。该运算也分别叫做<strong>最大池化或平均池化</strong>。<br><img src="https://imgvideoforblog.oss-cn-shanghai.aliyuncs.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202020-02-07%20%E4%B8%8B%E5%8D%885.50.41.png" alt="alt"></p>
<p>再次回到本节开始提到的物体边缘检测的例子。现在我们将卷积层的输出作为2×2最大池化的输入。设该卷积层输入是X、池化层输出为Y。无论是X[i, j]和X[i, j+1]值不同，还是X[i, j+1]和X[i, j+2]不同，池化层输出均有Y[i, j]=1（即识别到边缘）。也就是说，<em>使用2×2最大池化层时，只要卷积层识别的模式在高和宽上移动不超过一个元素，我们依然可以将它检测出来</em>。从而，降低了卷积运算对位置的过度敏感性。</p>
<p>下面把池化层的前向计算实现在pool2d函数里。它跟之前（二维卷积层）里corr2d函数非常类似，唯一的区别在计算输出Y上。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pool2d</span><span class="params">(X, pool_size, mode=<span class="string">'max'</span>)</span>:</span></span><br><span class="line">    X = X.float()</span><br><span class="line">    p_h, p_w = pool_size</span><br><span class="line">    Y = torch.zeros(X.shape[<span class="number">0</span>] - p_h + <span class="number">1</span>, X.shape[<span class="number">1</span>] - p_w + <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(Y.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(Y.shape[<span class="number">1</span>]):</span><br><span class="line">            <span class="keyword">if</span> mode == <span class="string">'max'</span>:</span><br><span class="line">                Y[i, j] = X[i: i + p_h, j: j + p_w].max() <span class="comment">#取输入中这一块矩阵中元素的最大值</span></span><br><span class="line">            <span class="keyword">elif</span> mode == <span class="string">'avg'</span>:</span><br><span class="line">                Y[i, j] = X[i: i + p_h, j: j + p_w].mean() <span class="comment">#取输入中这一块矩阵的平均值      </span></span><br><span class="line">    <span class="keyword">return</span> Y</span><br></pre></td></tr></table></figure>

<p>我们可以直接调用<code>nn.MaxPool2d</code>进行最大池化。<br>第一个参数是池化窗口的规模，我们还可以手动指定步幅和填充。（默认情况下，MaxPool2d实例里步幅和池化窗口形状相同。）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pool2d = nn.MaxPool2d(<span class="number">3</span>, padding=<span class="number">1</span>, stride=<span class="number">2</span>)</span><br><span class="line">pool2d(X)</span><br></pre></td></tr></table></figure>
<br>

<p>虽然池化和卷积都是这种”窗口滑动运算”的感觉，但区别在于，<strong>对于多输入通道，池化是不会将得到的多个结果进行叠加的，所以输入通道数=输出通道数</strong>。</p>
<Br>

<h1 id="LeNet"><a href="#LeNet" class="headerlink" title="LeNet"></a>LeNet</h1><p>本节里我们将介绍一个早期用来识别手写数字图像的卷积神经网络：LeNet [1]。<br>这个名字来源于LeNet论文的第一作者Yann LeCun。LeNet展示了通过梯度下降训练卷积神经网络可以达到手写数字识别在当时最先进的结果。这个奠基性的工作第一次将卷积神经网络推上舞台，为世人所知。LeNet的网络结构如下图所示。<br><img src="https://imgvideoforblog.oss-cn-shanghai.aliyuncs.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202020-02-07%20%E4%B8%8B%E5%8D%886.15.32.png" alt="alt"></p>
<p>LeNet分为卷积层块和全连接层块两个部分：</p>
<h2 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h2><p>基本单位是卷积层+最大池化层。卷积层用来识别图像里的空间模式，如线条和物体局部，之后的最大池化层则用来降低卷积层对位置的敏感性。</p>
<p>卷积层块由两个这样的基本单位重复堆叠构成。在卷积层块中，每个卷积层都使用5×55×5的窗口，并在输出上使用sigmoid激活函数。第一个卷积层输出通道数为6，第二个卷积层输出通道数则增加到16。这是因为第二个卷积层比第一个卷积层的输入的高和宽要小，所以增加输出通道使两个卷积层的参数尺寸类似。卷积层块的两个最大池化层的窗口形状均为2×22×2，且步幅为2。由于池化窗口与步幅形状相同，池化窗口在输入上每次滑动所覆盖的区域互不重叠。</p>
<h2 id="全连接层"><a href="#全连接层" class="headerlink" title="全连接层"></a>全连接层</h2><p>卷积层块的输出形状为(批量大小, 通道, 高, 宽)。<br>当卷积层块的输出传入全连接层块时，全连接层块会将小批量中每个样本变平（flatten）。也就是说，全连接层的输入形状将变成二维，其中第一维是小批量中的样本，第二维是每个样本变平后的向量表示。</p>
<p>全连接层块含3个全连接层。它们的输出个数分别是120、84和10，其中10为输出的类别个数。</p>
<h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><p>采用Sequential类来实现LeNet模型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn,optim</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">".."</span>)</span><br><span class="line"><span class="keyword">import</span> d2lzh_pytorch <span class="keyword">as</span> d2l</span><br><span class="line">device = torch.device(<span class="string">'cuda'</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">'cpu'</span>)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LeNet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(LeNet, self).__init__()</span><br><span class="line">        self.conv = nn.Sequential(</span><br><span class="line">                    nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, <span class="number">5</span>), <span class="comment"># in_channels, out_channels, kernel_size</span></span><br><span class="line">                    nn.Sigmoid(),</span><br><span class="line">                    nn.MaxPool2d(<span class="number">2</span>,<span class="number">2</span>) <span class="comment">#kernel_size, stride</span></span><br><span class="line">                    nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</span><br><span class="line">                    nn.Sigmoid(),</span><br><span class="line">                    nn.MaxPool2d(<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line">                )</span><br><span class="line">        self.fc = nn.Sequential(</span><br><span class="line">                    nn.Linear(<span class="number">16</span>*<span class="number">4</span>*<span class="number">4</span>, <span class="number">120</span>), <span class="comment">#第一个全连接层，输入维度为16个通道的4*4，输出为120个神经元</span></span><br><span class="line">                    nn.Sigmoid(),</span><br><span class="line">                    nn.Linear(<span class="number">120</span>, <span class="number">84</span>),</span><br><span class="line">                    nn.Sigmoid(),</span><br><span class="line">                    nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line">                )</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forwarfd</span><span class="params">(self, img)</span>:</span></span><br><span class="line">        feature = self.conv(img)</span><br><span class="line">        output = self.fc(feature.view(img.shape[<span class="number">0</span>], <span class="number">-1</span>)) <span class="comment">#把上一步得到的结果按样本为单位排，这样每个的维度才是16*4*4。</span></span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<p>可以看到，在卷积层块中输入的高和宽在逐层减小。卷积层由于使用高和宽均为5的卷积核，从而将高和宽分别减小4，而池化层则将高和宽减半，但通道数则从1增加到16。全连接层则逐层减少输出个数，直到变成图像的类别数10。</p>
<p>下面我们来实验LeNet模型。实验中，我们仍然使用Fashion-MNIST作为训练数据集。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size=batch_size)</span><br></pre></td></tr></table></figure>

<p>因为卷积神经网络计算比多层感知机要复杂，建议使用GPU来加速计算。因此，我们对3.6节（softmax回归的从零开始实现）中描述的evaluate_accuracy函数略作修改，使其支持GPU计算。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 本函数已保存在d2lzh_pytorch包中方便以后使用。该函数将被逐步改进。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate_accuracy</span><span class="params">(data_iter, net, device=None)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> device <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">and</span> isinstance(net, torch.nn.Module):</span><br><span class="line">        <span class="comment"># 如果没指定device就使用net的device</span></span><br><span class="line">        device = list(net.parameters())[<span class="number">0</span>].device</span><br><span class="line">    acc_sum, n = <span class="number">0.0</span>, <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">            <span class="keyword">if</span> isinstance(net, torch.nn.Module): <span class="comment">#net是继承于nn.Module的模型类的实例</span></span><br><span class="line">                net.eval() <span class="comment"># 评估模式, 这会关闭dropout， 对本batch的样本进行acc计算并保存到acc_sum</span></span><br><span class="line">                acc_sum += (net(X.to(device)).argmax(dim=<span class="number">1</span>) == y.to(device)).float().sum().cpu().item()</span><br><span class="line">                net.train() <span class="comment"># 改回训练模式</span></span><br><span class="line">            </span><br><span class="line">    <span class="keyword">return</span> acc_sum / n</span><br></pre></td></tr></table></figure>

<p>同样对之前定义过的train_ch3函数略作修改，确保计算使用的数据和模型同在内存或显存上：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 本函数已保存在d2lzh_pytorch包中方便以后使用</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_ch5</span><span class="params">(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)</span>:</span></span><br><span class="line">    net = net.to(device)</span><br><span class="line">    print(<span class="string">"training on "</span>, device)</span><br><span class="line">    loss = torch.nn.CrossEntropyLoss()</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        train_l_sum, train_acc_sum, n, batch_count, start = <span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0</span>, <span class="number">0</span>, time.time()</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">            X = X.to(device)</span><br><span class="line">            y = y.to(device)</span><br><span class="line">            y_hat = net(X)</span><br><span class="line">            l = loss(y_hat, y)</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            l.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line">            train_l_sum += l.cpu().item()</span><br><span class="line">            train_acc_sum += (y_hat.argmax(dim=<span class="number">1</span>) == y).sum().cpu().item()</span><br><span class="line">            n += y.shape[<span class="number">0</span>]</span><br><span class="line">            batch_count += <span class="number">1</span></span><br><span class="line">        test_acc = evaluate_accuracy(test_iter, net)</span><br><span class="line">        print(<span class="string">'epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec'</span></span><br><span class="line">              % (epoch + <span class="number">1</span>, train_l_sum / batch_count, train_acc_sum / n, test_acc, time.time() - start))</span><br></pre></td></tr></table></figure>

<p>学习率采用0.001，训练算法使用Adam算法，损失函数使用交叉熵损失函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">lr, num_epochs = <span class="number">0.001</span>, <span class="number">5</span></span><br><span class="line">optimizer = torch.optim.Adam(net.parameters(), lr=lr)</span><br><span class="line"></span><br><span class="line">train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">training on  cuda</span><br><span class="line">epoch <span class="number">1</span>, loss <span class="number">0.0072</span>, train acc <span class="number">0.322</span>, test acc <span class="number">0.584</span>, time <span class="number">3.7</span> sec</span><br><span class="line">epoch <span class="number">2</span>, loss <span class="number">0.0037</span>, train acc <span class="number">0.649</span>, test acc <span class="number">0.699</span>, time <span class="number">1.8</span> sec</span><br><span class="line">epoch <span class="number">3</span>, loss <span class="number">0.0030</span>, train acc <span class="number">0.718</span>, test acc <span class="number">0.724</span>, time <span class="number">1.7</span> sec</span><br><span class="line">epoch <span class="number">4</span>, loss <span class="number">0.0027</span>, train acc <span class="number">0.741</span>, test acc <span class="number">0.746</span>, time <span class="number">1.6</span> sec</span><br><span class="line">epoch <span class="number">5</span>, loss <span class="number">0.0024</span>, train acc <span class="number">0.759</span>, test acc <span class="number">0.759</span>, time <span class="number">1.7</span> sec</span><br></pre></td></tr></table></figure>
<Br>

<h1 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h1><p>神经网络可以直接基于图像的原始像素进行分类。这种称为<strong>端到端（end-to-end）</strong>的方法节省了很多中间步骤。然而，最好还是先提取好特征，再作为输入。这类图像分类研究的主要流程是：</p>
<p>1.获取图像数据集；<br>2.使用已有的<strong>特征提取函数</strong>生成图像的特征；<br>3.使用机器学习模型对图像的特征分类。</p>
<p>2012年，AlexNet横空出世。这个模型的名字来源于论文第一作者的姓名Alex Krizhevsky 。AlexNet使用了8层卷积神经网络，并以很大的优势赢得了ImageNet 2012图像识别挑战赛。它首次证明了<strong>学习到的特征</strong>可以超越手工设计的特征，从而一举打破计算机视觉研究的前状。<br><img src="https://imgvideoforblog.oss-cn-shanghai.aliyuncs.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202020-02-07%20%E4%B8%8B%E5%8D%887.13.29.png" alt="alt"></p>
<p>AlexNet与LeNet的设计理念非常相似，但也有显著的区别。</p>
<p>第一，与相对较小的LeNet相比，AlexNet包含8层变换，其中有<strong>5层卷积和2层全连接隐藏层，以及1个全连接输出层。</strong>下面我们来详细描述这些层的设计。</p>
<p>AlexNet第一层中的卷积窗口形状是11×11。因为ImageNet中绝大多数图像的高和宽均比MNIST图像的高和宽大10倍以上，ImageNet图像的物体占用更多的像素，所以需要更大的卷积窗口来捕获物体。第二层中的卷积窗口形状减小到5×5，之后全采用3×3。此外，第一、第二和第五个卷积层之后都使用了窗口形状为3×3、步幅为2的最大池化层。而且，AlexNet使用的卷积通道数也大于LeNet中的卷积通道数数十倍。</p>
<p>紧接着最后一个卷积层的是两个输出个数为4096的全连接层。这两个巨大的全连接层带来将近1 GB的模型参数。由于早期显存的限制，最早的AlexNet使用双数据流的设计使一个GPU只需要处理一半模型。幸运的是，显存在过去几年得到了长足的发展，因此通常我们不再需要这样的特别设计了。</p>
<p>第二，<strong>AlexNet将sigmoid激活函数改成了更加简单的ReLU激活函数</strong>。一方面，ReLU激活函数的计算更简单，例如它并没有sigmoid激活函数中的求幂运算。另一方面，ReLU激活函数在不同的参数初始化方法下使模型更容易训练。这是由于当sigmoid激活函数输出极接近0或1时，这些区域的梯度几乎为0，从而造成反向传播无法继续更新部分模型参数；而ReLU激活函数在正区间的梯度恒为1。因此，若模型参数初始化不当，sigmoid函数可能在正区间得到几乎为0的梯度，从而令模型无法得到有效训练。</p>
<p>第三，AlexNet通过<strong>丢弃法</strong>来控制全连接层的模型复杂度。而LeNet并没有使用丢弃法。</p>
<p>第四，AlexNet引入了<strong>大量的图像增广</strong>，如翻转、裁剪和颜色变化，从而进一步扩大数据集来缓解过拟合。</p>
<p>下面我们实现稍微简化过的AlexNet。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn, optim</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">".."</span>) </span><br><span class="line"><span class="keyword">import</span> d2lzh_pytorch <span class="keyword">as</span> d2l</span><br><span class="line">device = torch.device(<span class="string">'cuda'</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">'cpu'</span>)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AlexNet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(AlexNet, self).__init__()</span><br><span class="line">        self.conv = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">1</span>, <span class="number">96</span>, <span class="number">11</span>, <span class="number">4</span>), <span class="comment"># in_channels, out_channels, kernel_size, stride, padding</span></span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">3</span>, <span class="number">2</span>), <span class="comment"># kernel_size, stride</span></span><br><span class="line">            <span class="comment"># 减小卷积窗口，使用填充为2来使得输入与输出的高和宽一致，且增大输出通道数</span></span><br><span class="line">            nn.Conv2d(<span class="number">96</span>, <span class="number">256</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">3</span>, <span class="number">2</span>),</span><br><span class="line">            <span class="comment"># 连续3个卷积层，且使用更小的卷积窗口。除了最后的卷积层外，进一步增大了输出通道数。</span></span><br><span class="line">            <span class="comment"># 前两个卷积层后不使用池化层来减小输入的高和宽</span></span><br><span class="line">            nn.Conv2d(<span class="number">256</span>, <span class="number">384</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Conv2d(<span class="number">384</span>, <span class="number">384</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Conv2d(<span class="number">384</span>, <span class="number">256</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">        )        </span><br><span class="line">        </span><br><span class="line">         <span class="comment"># 这里全连接层的输出个数比LeNet中的大数倍。使用丢弃层来缓解过拟合</span></span><br><span class="line">        self.fc = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">256</span>*<span class="number">5</span>*<span class="number">5</span>, <span class="number">4096</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">            nn.Linear(<span class="number">4096</span>, <span class="number">4096</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">            <span class="comment"># 输出层。由于这里使用Fashion-MNIST，所以用类别数为10，而非论文中的1000</span></span><br><span class="line">            nn.Linear(<span class="number">4096</span>, <span class="number">10</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, img)</span>:</span></span><br><span class="line">        feature = self.conv(img)</span><br><span class="line">        output = self.fc(feature.view(img.shape[<span class="number">0</span>], <span class="number">-1</span>))</span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<br>


<p><strong>读取数据</strong>：<br>这里在获取数据之前，首先保存两个操作——扩大图像高宽+ToTensor()。<br>这里学到一个点：<em>用Compose方法将这两个对图像的预处理操作合成一组操作，以参数形式传入获取数据的函数中。</em></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data_fashion_mnist</span><span class="params">(batch_size, resize=None, root=<span class="string">'~/Datasets/FashionMNIST'</span>)</span>:</span></span><br><span class="line">    trans = [] <span class="comment">#操作数组</span></span><br><span class="line">    <span class="keyword">if</span> resize:</span><br><span class="line">        trans.append(torchvision.transforms.Resize(size=resize)) <span class="comment">#resize是要重塑的规模</span></span><br><span class="line">    trans.append(torchvision.transforms.ToTensor()) <span class="comment">#再加一个ToTensor操作</span></span><br><span class="line">    </span><br><span class="line">    transform = torchvision.transforms.Compose(trans)</span><br><span class="line">    mnist_train = torchvision.datasets.FashionMNIST(root=root, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">    mnist_test = torchvision.datasets.FashionMNIST(root=root, train=<span class="literal">False</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#定义数据集的DataLoader</span></span><br><span class="line">    train_iter = torch.utils.data.Dataloader(mnist_train, batch_size=batch_size, shuffle=<span class="literal">True</span>, num_workers=<span class="number">4</span>)</span><br><span class="line">    test_iter = torch.utils.data.Dataloader(mnist_train, batch_size=batch_size, shuffle=<span class="literal">True</span>, num_workers=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> train_iter, test_iter</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 如出现“out of memory”的报错信息，可减小batch_size或resize</span></span><br><span class="line">train_iter, test_iter = load_data_fashion_mnist(batch_size, resize=<span class="number">224</span>)</span><br></pre></td></tr></table></figure>

<p>训练：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">lr, num_epochs = <span class="number">0.001</span>, <span class="number">5</span></span><br><span class="line">optimizer = torch.optim.Adam(net.parameters(), lr=lr)</span><br><span class="line">d2l.train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">training on  cuda</span><br><span class="line">epoch <span class="number">1</span>, loss <span class="number">0.0047</span>, train acc <span class="number">0.770</span>, test acc <span class="number">0.865</span>, time <span class="number">128.3</span> sec</span><br><span class="line">epoch <span class="number">2</span>, loss <span class="number">0.0025</span>, train acc <span class="number">0.879</span>, test acc <span class="number">0.889</span>, time <span class="number">128.8</span> sec</span><br><span class="line">epoch <span class="number">3</span>, loss <span class="number">0.0022</span>, train acc <span class="number">0.898</span>, test acc <span class="number">0.901</span>, time <span class="number">130.4</span> sec</span><br><span class="line">epoch <span class="number">4</span>, loss <span class="number">0.0019</span>, train acc <span class="number">0.908</span>, test acc <span class="number">0.900</span>, time <span class="number">131.4</span> sec</span><br><span class="line">epoch <span class="number">5</span>, loss <span class="number">0.0018</span>, train acc <span class="number">0.913</span>, test acc <span class="number">0.902</span>, time <span class="number">129.9</span> sec</span><br></pre></td></tr></table></figure>

<Br>

<h1 id="VGG"><a href="#VGG" class="headerlink" title="VGG"></a>VGG</h1><p>VGG提出了可以通过<strong>重复使用简单的基础块</strong>来构建深度模型的思路。</p>
<p><em>我的小结：VGG分为卷积部分和全连接部分，卷积部分包括多个卷积模块，每个卷积模块又可包含1个或多个卷积层（最后会接一个激活层+一个maxpooling层）。<br>其中，全连接层stride=2使得高和宽减半，而out_channels的设置会使通道翻倍，</em></p>
<p>VGG块的组成规律是：连续使用多个填充为1、窗口形状为3×3的卷积层，再用一个步幅为2、窗口形状为2×2的最大池化层。<br>卷积层保持输入的高和宽不变，而池化层则对其减半。我们使用vgg_block函数来实现这个基础的VGG块，它可以指定卷积层的数量和输入输出通道数。</p>
<blockquote>
<p>对于给定的感受野（与输出有关的输入图片的局部大小），采用堆积的小卷积核优于采用大的卷积核，因为可以增加网络深度来保证学习更复杂的模式，而且代价还比较小（参数更少）。例如，在VGG中，使用了3个3x3卷积核来代替7x7卷积核，使用了2个3x3卷积核来代替5*5卷积核，这样做的主要目的是在保证具有相同感知野的条件下，提升了网络的深度，在一定程度上提升了神经网络的效果。</p>
</blockquote>
<p>以下实现一个vgg block的定义，注意参数num_convs表示该模块中有多少个卷积层。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn,optim</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">".."</span>) </span><br><span class="line"><span class="keyword">import</span> d2lzh_pytorch <span class="keyword">as</span> d2l</span><br><span class="line">device = torch.device(<span class="string">'cuda'</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">'cpu'</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">vgg_block</span><span class="params">(num_convs, in_channels, out_channels)</span>:</span></span><br><span class="line">    blk = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_convs): <span class="comment">#num_convs个卷积层</span></span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">            blk.append(nn.Conv2d(in_channels, out_channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment">#则是多卷积层的模块，除了第一个，之后的卷积层都不要改变输出维度。</span></span><br><span class="line">            blk.append(nn.Conv2d(out_channels, out_channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>))</span><br><span class="line">        <span class="comment">#每个卷积层后加个激活函数</span></span><br><span class="line">        blk.append(nn.ReLU())</span><br><span class="line">        </span><br><span class="line">    <span class="comment">#最后加个池化层</span></span><br><span class="line">    blk.append(nn.Maxpool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)) <span class="comment">#这里会使宽高减半</span></span><br><span class="line">            </span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(*blk)  <span class="comment">#放到Sequential中创建模型类</span></span><br><span class="line">    <span class="comment">#注意加*号，即将blk内容转换成一个元组</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>星号变量是用在函数的参数传递上的，在下面的实例中，单个星号代表这个位置接收任意多个非关键字参数，在函数的<em>b位置上将其转化成元组，而双星号代表这个位置接收任意多个关键字参数，在*</em>b位置上将其转化成字典：</p>
<p> *：该位置接受任意多个非关键字（non-keyword）参数，在函数中将其转化为元组（1,2,3,4）</p>
<p>**： 该位置接受任意多个关键字（keyword）参数，转化为词典 [key:value, key:value ]</p>
</blockquote>
<Br>

<p><strong>VGG网络</strong><br>现在我们构造一个VGG网络。它有5个卷积块，前2块使用单卷积层，而后3块使用双卷积层。第一块的输入输出通道分别是1（因为下面要使用的Fashion-MNIST数据的通道数为1）和64，之后每次对输出通道数翻倍，直到变为512。因为这个网络使用了8个卷积层和3个全连接层，所以经常被称为VGG-11。<br>conv_arch参数指定了<em>一个模块中有多少个卷积层、模块的输入通道数和输出通道数</em>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">conv_arch = ((<span class="number">1</span>, <span class="number">1</span>, <span class="number">64</span>), (<span class="number">1</span>, <span class="number">64</span>, <span class="number">128</span>), (<span class="number">2</span>, <span class="number">128</span>, <span class="number">256</span>), (<span class="number">2</span>, <span class="number">256</span>, <span class="number">512</span>), (<span class="number">2</span>, <span class="number">512</span>, <span class="number">512</span>))</span><br><span class="line"><span class="comment"># 经过5个vgg_block, 宽高会减半5次, 变成 224/32 = 7</span></span><br><span class="line">fc_features = <span class="number">512</span> * <span class="number">7</span> * <span class="number">7</span> <span class="comment"># c * w * h</span></span><br><span class="line">fc_hidden_units = <span class="number">4096</span> <span class="comment"># 任意</span></span><br></pre></td></tr></table></figure>

<p>实现VGG-11：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">vgg</span><span class="params">(conv_arch, fc_features, fc_hidden_units=<span class="number">4096</span>)</span>:</span></span><br><span class="line">    net = nn.Sequential((</span><br><span class="line">    <span class="comment">#卷积层部分</span></span><br><span class="line">        <span class="keyword">for</span> idx, (num_convs, in_channels, out_channels) <span class="keyword">in</span> enumerate(conv_arch):</span><br><span class="line">            net.add_module(<span class="string">"vgg_block"</span>+str(idx+<span class="number">1</span>), vgg_block(num_convs, in_channels, out_channels))</span><br><span class="line">    <span class="comment">#全连接层部分</span></span><br><span class="line">        net.add_module(<span class="string">"fc"</span>, nn.Sequential(d2l.FlattenLayer(),</span><br><span class="line">                                 nn.Linear(fc_features, fc_hidden_units),</span><br><span class="line">                                 nn.ReLU(),</span><br><span class="line">                                 nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">                                 nn.Linear(fc_hidden_units, fc_hidden_units),</span><br><span class="line">                                 nn.ReLU(),</span><br><span class="line">                                 nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">                                 nn.Linear(fc_hidden_units, <span class="number">10</span>)</span><br><span class="line">                                ))</span><br><span class="line">    <span class="keyword">return</span> net</span><br></pre></td></tr></table></figure>
<p>下面构造一个高和宽均为224的单通道数据样本来观察每一层的输出形状。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">net = vgg(conv_arch, fc_features, fc_hidden_units)</span><br><span class="line">X = torch.rand(<span class="number">1</span>,<span class="number">1</span>,<span class="number">224</span>,<span class="number">224</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># named_children获取一级子模块及其名字(named_modules会返回所有子模块,包括子模块的子模块)</span></span><br><span class="line"><span class="keyword">for</span> name, blk <span class="keyword">in</span> net.named_children():</span><br><span class="line">    X = blk(X) <span class="comment">#相当于自设forward</span></span><br><span class="line">    print(name, <span class="string">'output shape:'</span>, X.shape)</span><br></pre></td></tr></table></figure>
<p>named_children获取一级子模块及其名字(named_modules会返回所有子模块,包括子模块的子模块)。这里，显然，根据conv_arch参数的设置，有5个卷积模块，1个全连接模块。</p>
<p>输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">vgg_block_1 output shape:  torch.Size([<span class="number">1</span>, <span class="number">64</span>, <span class="number">112</span>, <span class="number">112</span>])</span><br><span class="line">vgg_block_2 output shape:  torch.Size([<span class="number">1</span>, <span class="number">128</span>, <span class="number">56</span>, <span class="number">56</span>])</span><br><span class="line">vgg_block_3 output shape:  torch.Size([<span class="number">1</span>, <span class="number">256</span>, <span class="number">28</span>, <span class="number">28</span>])</span><br><span class="line">vgg_block_4 output shape:  torch.Size([<span class="number">1</span>, <span class="number">512</span>, <span class="number">14</span>, <span class="number">14</span>])</span><br><span class="line">vgg_block_5 output shape:  torch.Size([<span class="number">1</span>, <span class="number">512</span>, <span class="number">7</span>, <span class="number">7</span>])</span><br><span class="line">fc output shape:  torch.Size([<span class="number">1</span>, <span class="number">10</span>])</span><br></pre></td></tr></table></figure>
<p>可以看到，每次我们将输入的高和宽减半，直到最终高和宽变成7后传入全连接层。与此同时，输出通道数每次翻倍，直到变成512。因为每个卷积层的窗口大小一样，所以每层的模型参数尺寸和计算复杂度与输入高、输入宽、输入通道数和输出通道数的乘积成正比。VGG这种<strong>高和宽减半以及通道翻倍的设计</strong>使得多数卷积层都有相同的模型参数尺寸和计算复杂度。</p>
<br>

<h1 id="NiN"><a href="#NiN" class="headerlink" title="NiN"></a>NiN</h1><p>之前介绍的LeNet、AlexNet和VGG在设计上的共同之处是：先以由卷积层构成的模块充分抽取空间特征，再以由全连接层构成的模块来输出分类结果。其中，<strong>AlexNet和VGG对LeNet的改进主要在于如何对这两个模块加宽（增加通道数）和加深</strong>。本节我们介绍网络中的网络（NiN）[1]。它提出了另外一个思路，即串联多个由卷积层和“全连接”层构成的小网络来构建一个深层网络。</p>
<p><em>回顾一下，VGG的设计是分为“卷积层部分”和“全连接层部分”，那么，这个逻辑就是，全连接层之后无法再接卷积层来处理了。因为如果想在全连接层之后接卷积层的话，要考虑维度，将全连接层输出从（样本，特征）变成四维数组（样本，通道，高，宽），从而适应卷积层的输入形式。这样必然复杂化了，而且全连接层已经得到分类结果了还去处理个啥。<br>这里，NiN是希望在全连接层之后再接卷积层的，即把这个网络换个形式“加深”。这里便用到“1X1卷积层”，抽象地理解，它可以看成全连接层，其中空间维度（高和宽）上的每个元素相当于样本，通道相当于特征。因此，NiN使用1×11×1卷积层来替代全连接层，从而使空间信息能够自然传递到后面的层中去。</em></p>
<p>图5.7对比了NiN同AlexNet和VGG等网络在结构上的主要区别。<br><img src="https://imgvideoforblog.oss-cn-shanghai.aliyuncs.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202020-02-08%20%E4%B8%8B%E5%8D%888.58.27.png" alt="alt"></p>
<p>NiN块是NiN中的基础块。它由一个卷积层加两个充当全连接层的1×11×1卷积层串联而成。其中第一个卷积层的超参数可以自行设置，而第二和第三个1X1卷积层（模拟全连接层）的超参数一般是固定的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn, optim</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">".."</span>) </span><br><span class="line"><span class="keyword">import</span> d2lzh_pytorch <span class="keyword">as</span> d2l</span><br><span class="line">device = torch.device(<span class="string">'cuda'</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">'cpu'</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nin_block</span><span class="params">(in_channels, out_channels, kernel_size, stride, padding)</span>:</span></span><br><span class="line">    blk = nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),</span><br><span class="line">                        nn.ReLU(),</span><br><span class="line">                        nn.Conv2d(out_channels, out_channels, kernel_size=<span class="number">1</span>),</span><br><span class="line">                        nn.ReLU(),</span><br><span class="line">                        nn.Conv2d(out_channels, out_channels, kernel_size=<span class="number">1</span>),</span><br><span class="line">                        nn.ReLU())</span><br><span class="line">    <span class="keyword">return</span> blk</span><br></pre></td></tr></table></figure>
<br>


<p>除使用NiN块以外，NiN还有一个设计与AlexNet显著不同：NiN去掉了AlexNet最后的3个全连接层，那么怎么来得到分类结果呢？</p>
<p>NiN最后使用了输出通道数等于标签类别数的NiN块，然后使用<strong>全局平均池化层</strong>对每个通道中所有元素求平均，那么每一维得到一个平均数，即得到了标签类别数个平均数，将其直接作为分类结果。</p>
<p>这里的全局平均池化层即窗口形状等于输入空间维形状的平均池化层——也就是大小等同于样本每个通道的高X宽，毕竟一维池化出来就一个数嘛。NiN的这个设计的好处是可以显著减小模型参数尺寸，从而缓解过拟合。然而，该设计有时会造成获得有效模型的训练时间的增加。</p>
<p>全局平均池化层类：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GlobalAvgPool2d</span><span class="params">(nn.Module)</span>:</span> <span class="comment">#自定义全局平均池化层</span></span><br><span class="line">    <span class="comment"># 全局平均池化层可通过将池化窗口形状设置成输入的高和宽实现</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(GlobalAvgPool2d, self).__init__()</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> F.avg_pool2d(x, kernel_size=x.size()[<span class="number">2</span>:])</span><br></pre></td></tr></table></figure>
<p>可见，二维平均池化还是调用torch.nn.Functional工具包里的函数avg_pool2d。只是这里要想以“通道数”为单位来进行平均池化，所以取池化窗口大小为样本每个通道的高X宽。</p>
<p>网络模型类：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(</span><br><span class="line">    nin_block(<span class="number">1</span>, <span class="number">96</span>, kernel_size=<span class="number">11</span>, stride=<span class="number">4</span>, padding=<span class="number">0</span>),</span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nin_block(<span class="number">96</span>, <span class="number">256</span>, kernel_size=<span class="number">5</span>, stride=<span class="number">1</span>, padding=<span class="number">2</span>),</span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nin_block(<span class="number">256</span>, <span class="number">384</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>), </span><br><span class="line">    nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">    <span class="comment"># 标签类别数是10</span></span><br><span class="line">    nin_block(<span class="number">384</span>, <span class="number">10</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">    GlobalAvgPool2d(),</span><br><span class="line">    d2l.FlattenLayer())</span><br><span class="line">    )</span><br></pre></td></tr></table></figure>

<p>获取数据和训练：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">128</span></span><br><span class="line"><span class="comment"># 如出现“out of memory”的报错信息，可减小batch_size或resize</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=<span class="number">224</span>)</span><br><span class="line"></span><br><span class="line">lr, num_epochs = <span class="number">0.002</span>, <span class="number">5</span></span><br><span class="line">optimizer = torch.optim.Adam(net.parameters(), lr=lr)</span><br><span class="line">d2l.train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)</span><br></pre></td></tr></table></figure>

<h1 id="GoogleNet"><a href="#GoogleNet" class="headerlink" title="GoogleNet"></a>GoogleNet</h1><p>在2014年的ImageNet图像识别挑战赛中，一个名叫GoogLeNet的网络结构大放异彩。</p>
<p>跟之前所讲的相同，也是拥有专属设计的基础卷积块，叫做Inception块，得名于同名电影《盗梦空间》（Inception）。<br><img src="https://imgvideoforblog.oss-cn-shanghai.aliyuncs.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202020-02-08%20%E4%B8%8B%E5%8D%889.31.27.png" alt="alt"></p>
<p>Inception块中可以自定义的超参数是每个层的输出通道数，我们以此来控制模型复杂度。<br><strong>我们得到这四条线路的输出结果后，使用torch.cat函数作为不同的维度进行拼接（dim=1 不是行哦是通道维），由此作为一个Inception块的输出。</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn, optim</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">".."</span>) </span><br><span class="line"><span class="keyword">import</span> d2lzh_pytorch <span class="keyword">as</span> d2l</span><br><span class="line">device = torch.device(<span class="string">'cuda'</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">'cpu'</span>)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Inception</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="comment"># c1 - c4为每条线路里的层的输出通道数</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_c, c1, c2, c3, c4)</span>:</span></span><br><span class="line">        super(Inception, self).__init__()</span><br><span class="line">        <span class="comment"># 线路1，单1 x 1卷积层</span></span><br><span class="line">        self.p1_1 = nn.Conv2d(in_c, c1, kernel_size=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 线路2，1 x 1卷积层后接3 x 3卷积层</span></span><br><span class="line">        self.p2_1 = nn.Conv2d(in_c, c2[<span class="number">0</span>], kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.p2_2 = nn.Conv2d(c2[<span class="number">0</span>], c2[<span class="number">1</span>], kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 线路3，1 x 1卷积层后接5 x 5卷积层</span></span><br><span class="line">        self.p3_1 = nn.Conv2d(in_c, c3[<span class="number">0</span>], kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.p3_2 = nn.Conv2d(c3[<span class="number">0</span>], c3[<span class="number">1</span>], kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 线路4，3 x 3最大池化层后接1 x 1卷积层</span></span><br><span class="line">        self.p4_1 = nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.p4_2 = nn.Conv2d(in_c, c4, kernel_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        p1 = F.relu(self.p1_1(x))</span><br><span class="line">        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))</span><br><span class="line">        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))</span><br><span class="line">        p4 = F.relu(self.p4_2(self.p4_1(x)))</span><br><span class="line">        <span class="keyword">return</span> torch.cat((p1, p2, p3, p4), dim=<span class="number">1</span>)  <span class="comment"># 在通道维上连结输出</span></span><br></pre></td></tr></table></figure>
<br>

<p>至于GoogLeNet网络模型的设计，是设计了5个模块，每个模块运用了一些卷积层或者Inception基础模块。其中Inception块的通道数分配之比是在ImageNet数据集上通过大量的实验得来的。具体设计细节在此忽略。</p>
<Br>

<h1 id="批量归一化"><a href="#批量归一化" class="headerlink" title="批量归一化"></a>批量归一化</h1><p>通常来说，数据标准化预处理对于浅层模型就足够有效了。随着模型训练的进行，当每层中参数更新时，靠近输出层的输出较难出现剧烈变化。但对深层神经网络来说，即使输入数据已做标准化，训练中模型参数的更新依然很容易造成靠近输出层输出的剧烈变化。这种计算数值的不稳定性通常令我们难以训练出有效的深度模型。</p>
<p>批量归一化的提出正是为了应对深度模型训练的挑战。<strong>在模型训练时，批量归一化利用小批量上的均值和标准差，不断调整神经网络中间输出，从而使整个神经网络在各层的中间输出的数值更稳定。</strong>批量归一化和下一节将要介绍的残差网络为训练和设计深度模型提供了两类重要思路。</p>
<h2 id="全连接层-1"><a href="#全连接层-1" class="headerlink" title="全连接层"></a>全连接层</h2><p>如何对全连接层做批量归一化？通常，我们对全连接层的输出结果进行批量归一化，然后再用激活函数。</p>
<p>不是有口诀嘛：BAD </p>
<p>批量归一化的公式就是-均值/标准差，然后再学习两个参数对它进行“逆操作”，具体流程如下：<br><img src="https://imgvideoforblog.oss-cn-shanghai.aliyuncs.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202020-02-08%20%E4%B8%8B%E5%8D%8810.37.11.png" alt="alt"></p>
<h2 id="卷积层-1"><a href="#卷积层-1" class="headerlink" title="卷积层"></a>卷积层</h2><p>对卷积层来说，批量归一化发生在卷积计算之后、应用激活函数之前。如果卷积计算输出多个通道，我们需要对这些通道的输出分别做批量归一化，且<strong>每个通道都拥有独立的拉伸和偏移参数</strong>，并均为标量。</p>
<p>设小批量中有m个样本。在单个通道上，假设卷积计算输出的高和宽分别为p和q。我们需要对该通道中m×p×q个元素同时做批量归一化。对这些元素做标准化计算时，我们使用相同的均值和方差，即该通道中m×p×q个元素的均值和方差。</p>
<h2 id="预测时"><a href="#预测时" class="headerlink" title="预测时"></a>预测时</h2><p>注意：一般我们只在训练时用BN，<strong>预测时</strong>如果要用的话，一种常用的方法是通过<strong>移动平均估算整个训练数据集的样本均值和方差</strong>，并在预测时使用它们得到确定的输出。</p>
<p>移动平均估算整个训练数据集的样本均值和方差？怎么理解呢？根据下面的代码，可以理解为，在训练的时候就将得到的mean和var以一定的遗忘率momentum保存为moving_mean和moving_var，每次在训练时调用BN都会执行此操作，直至最后在预测时BN直接用上moving_mean和moving_var。</p>
<br>


<p><strong>从零开始实现</strong>（请务必看此代码注释）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn, optim</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">".."</span>)</span><br><span class="line"><span class="keyword">import</span> d2lzh_pytorch <span class="keyword">as</span> d2l</span><br><span class="line">device = torch.device(<span class="string">'cuda'</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">'cpu'</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batch_norm</span><span class="params">(is_training, X, gamma, beta, moving_mean, moving_var, eps, momentum)</span>:</span></span><br><span class="line">    <span class="comment"># 判断当前模式是训练模式还是预测模式</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> is_training:</span><br><span class="line">        <span class="comment">#如果是在预测，直接使用传入的移动平均所得的均值和方差</span></span><br><span class="line">            X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment">#训练模式，分为卷积层BN或全连接层BN，区别在于操作的维度不同。</span></span><br><span class="line">        <span class="comment">#全连接层的输出就两个维度（样本，神经元数）</span></span><br><span class="line">        <span class="comment">#卷积层的输出有四个维度（样本，通道，高，宽）</span></span><br><span class="line">        <span class="keyword">assert</span> len(X.shape) <span class="keyword">in</span> (<span class="number">2</span>,<span class="number">4</span>)</span><br><span class="line">        <span class="keyword">if</span> len(X.shape) == <span class="number">2</span>: </span><br><span class="line">            <span class="comment">#全连接层BN：对每个神经元对应的输出值求平均和标准差</span></span><br><span class="line">            mean = X.mean(dim=<span class="number">0</span>)</span><br><span class="line">            var = ((X-mean)**<span class="number">2</span>).mean(dim=<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">else</span> : </span><br><span class="line">            <span class="comment">#卷积层BN：对卷积层输出的每个通道求平均值和标准差</span></span><br><span class="line">            mean = X.mean(dim=<span class="number">0</span>, keepdim=<span class="literal">True</span>).mean(dim=<span class="number">2</span>, keepdim=<span class="literal">True</span>).mean(dim=<span class="number">3</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">            var = ((X - mean)**<span class="number">2</span>).mean(dim=<span class="number">0</span>, keepdim=<span class="literal">True</span>).mean(dim=<span class="number">2</span>, keepdim=<span class="literal">True</span>).mean(dim=<span class="number">3</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment">#标准化</span></span><br><span class="line">        X_hat = (X-mean)/torch.sqrt(var+eps)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#更新移动平均的均值和方差</span></span><br><span class="line">        moving_mean = momentum * moving_mean + (<span class="number">1.0</span>-momentum)*mean <span class="comment">#这里的细节，1写成1.0 我惊了</span></span><br><span class="line">        moving_var = momentum * moving_var + (<span class="number">1.0</span>-momentum)*var</span><br><span class="line">        </span><br><span class="line">    Y = gamma * X_hat + beta <span class="comment">#拉伸与偏移</span></span><br><span class="line">    <span class="keyword">return</span> Y, moving_mean, moving_var</span><br></pre></td></tr></table></figure>
<p>这里重新回顾<strong>mean函数关于多维数组的操作，维度的理解</strong>，以前都是死记硬背的，这次理解住了：</p>
<blockquote>
<p>比如说，对于规模为（20,30,40）的三维数据，使用mean(dim=0)，即把第0维的20个数求平均成为一个数，得到20X30X40/20=30X40个数。<br>使用mean(dim=(0,1))，即把第0维和第1维的20X30=600个数求平均成为一个数，得到40个数。<br>所以回顾之前的死记硬背，之所以dim=0是以列求平均，是因为是把行数个数据（所以是竖着）求平均。<br>所以，对于卷积层BN对通道即第一维度进行求平均，则是mean(dim=(0,2,3))，得到的结果规模即第一维的规模，即每个通道一个平均数。<br>所以，其实<strong>对于多维数组的mean操作，思考其结果得到最终数据的规模，便可知道如何设dim参数操作</strong>。</p>
</blockquote>
<p>而<strong>keepdim=True</strong>，就是说，输入数据的维度是不变化的，就算是对某个维度求平均数了，也不是只得到一个数，而是<em>将该维度上的元素都变成这个数值</em>，从而保障维度保持不变，为的是这个结果之后的继续运算。</p>
<br>

<p>接下来，我们需要定义一个<strong>BN的网络层模型类</strong>。<br><em>这时候若你问，诶我不是运算函数都弄出来了，到时候直接算呗？注意，第一，函数的参数（网络中需要的学习的参数）gamma和beta这些参数你不先定义出来怎么学？第二，你要把这个BN作为一个层放到网络中，那你必须弄个网络模型类设计好前向传播。</em></p>
<p>BatchNorm实例所需指定的num_features参数对于全连接层来说应为输出神经元个数，对于卷积层来说则为输出通道数。（（以此才好设置gamma和beta参数的规模，毕竟需要拉伸和偏移））<br>该实例所需指定的num_dims参数对于全连接层和卷积层来说分别为2和4。然后从传入的num_dims就可以看出是全连接层BN还是卷积层BN。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BatchNorm</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_features, num_dims)</span>:</span></span><br><span class="line">        super(BatchNorm, self).__init__()</span><br><span class="line">        <span class="comment">#设置参数的规模</span></span><br><span class="line">        <span class="keyword">if</span> num_dims == <span class="number">2</span>:</span><br><span class="line">            shape = (<span class="number">1</span>, num_features)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            shape = (<span class="number">1</span>, num_features, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 参与求梯度和迭代的拉伸和偏移参数，分别初始化成0和1</span></span><br><span class="line">        self.gamma = nn.Parameter(torch.ones(shape))</span><br><span class="line">        self.beta = nn.Parameter(torch.zeros(shape))</span><br><span class="line">        <span class="comment">#不参与求梯度和迭代的变量，全在内存上初始化成0</span></span><br><span class="line">        self.moving_mean = torch.zeros(shape)</span><br><span class="line">        self.moving_var = torch.zeros(shape)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="comment"># 如果X不在内存上，将moving_mean和moving_var复制到X所在显存上</span></span><br><span class="line">        <span class="keyword">if</span> self.moving_mean.device != X.device:</span><br><span class="line">            self.moving_mean = self.moving_mean.to(X.device)</span><br><span class="line">            self.moving_var = self.moving_var.to(X.device)</span><br><span class="line">        <span class="comment"># 得到输出结果，以及保存的moving_mean和moving_var</span></span><br><span class="line">        Y, self.moving_mean, self.moving_var = batch_norm(self.training, X, self.gamma, self.beta, self.moving_mean, self.moving_var, eps=<span class="number">1e-5</span>, momentum=<span class="number">0.9</span>) </span><br><span class="line">        <span class="comment">#self.training默认为True,在调用.eval()后设为false</span></span><br><span class="line">    <span class="keyword">return</span> Y</span><br></pre></td></tr></table></figure>

<br>

<p>接下来，使用批量归一化层：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(</span><br><span class="line">        nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, <span class="number">5</span>), <span class="comment"># in_channels, out_channels, kernel_size</span></span><br><span class="line">        BatchNorm(<span class="number">6</span>, num_dims=<span class="number">4</span>), <span class="comment">#该卷积层输出的通道数为6</span></span><br><span class="line">        nn.Sigmoid(),</span><br><span class="line">        nn.MaxPool2d(<span class="number">2</span>,<span class="number">2</span>) <span class="comment">#kernel_size, stride</span></span><br><span class="line">        nn.Conv2d(<span class="number">6</span>,<span class="number">16</span>,<span class="number">5</span>),</span><br><span class="line">        BatchNorm(<span class="number">16</span>, num_dims=<span class="number">4</span>),</span><br><span class="line">        nn.Sigmoid(),</span><br><span class="line">        nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>),</span><br><span class="line">        d2l.FlattenLayer(),</span><br><span class="line">        nn.Linear(<span class="number">16</span>*<span class="number">4</span>*<span class="number">4</span>, <span class="number">120</span>),</span><br><span class="line">        BatchNorm(<span class="number">120</span>, num_dims=<span class="number">2</span>),</span><br><span class="line">        nn.Sigmoid(),</span><br><span class="line">        nn.Linear(<span class="number">120</span>, <span class="number">84</span>),</span><br><span class="line">        BatchNorm(<span class="number">84</span>, num_dims=<span class="number">2</span>),</span><br><span class="line">        nn.Sigmoid(),</span><br><span class="line">        nn.Linear(<span class="number">84</span>, <span class="number">10</span>),</span><br><span class="line">        )</span><br></pre></td></tr></table></figure>
<p>下面我们训练：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size=batch_size)</span><br><span class="line"></span><br><span class="line">lr, num_epochs = <span class="number">0.001</span>, <span class="number">5</span></span><br><span class="line">optimizer = torch.optim.Adam(net.parameters(), lr=lr)</span><br><span class="line">d2l.train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">training on  cuda</span><br><span class="line">epoch <span class="number">1</span>, loss <span class="number">0.0039</span>, train acc <span class="number">0.790</span>, test acc <span class="number">0.835</span>, time <span class="number">2.9</span> sec</span><br><span class="line">epoch <span class="number">2</span>, loss <span class="number">0.0018</span>, train acc <span class="number">0.866</span>, test acc <span class="number">0.821</span>, time <span class="number">3.2</span> sec</span><br><span class="line">epoch <span class="number">3</span>, loss <span class="number">0.0014</span>, train acc <span class="number">0.879</span>, test acc <span class="number">0.857</span>, time <span class="number">2.6</span> sec</span><br><span class="line">epoch <span class="number">4</span>, loss <span class="number">0.0013</span>, train acc <span class="number">0.886</span>, test acc <span class="number">0.820</span>, time <span class="number">2.7</span> sec</span><br><span class="line">epoch <span class="number">5</span>, loss <span class="number">0.0012</span>, train acc <span class="number">0.891</span>, test acc <span class="number">0.859</span>, time <span class="number">2.8</span> sec</span><br></pre></td></tr></table></figure>

<p>最后我们查看第一个批量归一化层学习到的拉伸参数gamma和偏移参数beta。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net[<span class="number">1</span>].gamma.view((<span class="number">-1</span>,)), net[<span class="number">1</span>].beta.view((<span class="number">-1</span>,))</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(tensor([ <span class="number">1.2537</span>,  <span class="number">1.2284</span>,  <span class="number">1.0100</span>,  <span class="number">1.0171</span>,  <span class="number">0.9809</span>,  <span class="number">1.1870</span>], device=<span class="string">'cuda:0'</span>),</span><br><span class="line"> tensor([ <span class="number">0.0962</span>,  <span class="number">0.3299</span>, <span class="number">-0.5506</span>,  <span class="number">0.1522</span>, <span class="number">-0.1556</span>,  <span class="number">0.2240</span>], device=<span class="string">'cuda:0'</span>))</span><br></pre></td></tr></table></figure>
<Br>

<p><strong>简洁实现</strong><br>采用工具函数<code>nn.BatchNorm2d(num_features)</code>。只需要指定全连接层输出的元素个数或卷积层输出的通道个数，不需要传入num_dims=2或4，可知该工具函数内部可以自动区分是全连接层BN还是卷积层BN。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, <span class="number">5</span>), <span class="comment"># in_channels, out_channels, kernel_size</span></span><br><span class="line">            nn.BatchNorm2d(<span class="number">6</span>),</span><br><span class="line">            nn.Sigmoid(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>), <span class="comment"># kernel_size, stride</span></span><br><span class="line">            nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">16</span>),</span><br><span class="line">            nn.Sigmoid(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>),</span><br><span class="line">            d2l.FlattenLayer(),</span><br><span class="line">            nn.Linear(<span class="number">16</span>*<span class="number">4</span>*<span class="number">4</span>, <span class="number">120</span>),</span><br><span class="line">            nn.BatchNorm1d(<span class="number">120</span>),</span><br><span class="line">            nn.Sigmoid(),</span><br><span class="line">            nn.Linear(<span class="number">120</span>, <span class="number">84</span>),</span><br><span class="line">            nn.BatchNorm1d(<span class="number">84</span>),</span><br><span class="line">            nn.Sigmoid(),</span><br><span class="line">            nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line">        )</span><br></pre></td></tr></table></figure>

<br>

<h1 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h1><p>残差网络（ResNet）</p>
<p>让我们先思考一个问题：在网络中添加新的层，会不会有效降低训练误差？我们从概念上认为，既然原模型解的空间只是新模型解的空间的子空间，那每新添加一层应该是可以降低更多训练误差的。然而在实践中，添加过多的层后训练误差往往不降反升。即使利用批量归一化带来的数值稳定性使训练深层模型更加容易，该问题仍然存在。</p>
<p>针对这一问题，何恺明等人提出了残差网络（ResNet） 。它在2015年的ImageNet图像识别挑战赛夺魁，并深刻影响了后来的深度神经网络的设计。</p>
<h2 id="残差块"><a href="#残差块" class="headerlink" title="残差块"></a>残差块</h2><p>在残差块中，输入可通过<strong>跨层的数据线路</strong>更快地向前传播。<br><img src="https://imgvideoforblog.oss-cn-shanghai.aliyuncs.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202020-02-09%20%E4%B8%8B%E5%8D%885.36.21.png" alt="alt"></p>
<p>ResNet沿用了VGG全3×3卷积层的设计。残差块里首先有2个有相同输出通道数的3×3卷积层。每个卷积层后接一个批量归一化层和ReLU激活函数。然后我们<strong>将输入跳过这两个卷积运算后直接加在最后的ReLU激活函数前</strong>。这样的设计要求两个卷积层的输出与输入形状一样，从而可以相加。如果想改变通道数，就需要引入一个额外的1×1卷积层来变换通道数后再做相加运算。</p>
<p>残差块的实现如下。它可以设定输出通道数、是否使用额外的1×1卷积层来修改通道数以及卷积层的步幅。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn, optim</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">".."</span>) </span><br><span class="line"><span class="keyword">import</span> d2lzh_pytorch <span class="keyword">as</span> d2l</span><br><span class="line">device = torch.device(<span class="string">'cuda'</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">'cpu'</span>)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Residual</span><span class="params">(nn.Module)</span>:</span>  <span class="comment"># 本类已保存在d2lzh_pytorch包中方便以后使用</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_channels, out_channels, use_1x1conv=False, stride=<span class="number">1</span>)</span>:</span></span><br><span class="line">        super(Residual, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, stride=stride)</span><br><span class="line">        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> use_1x1conv:</span><br><span class="line">            self.conv3 = nn.Conv2d(in_channels, out_channels, kernel_size=<span class="number">1</span>, stride=stride)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.conv3 = <span class="literal">None</span></span><br><span class="line">        self.bn1 = nn.BatchNorm2d(out_channels)</span><br><span class="line">        self.bn2 = nn.BatchNorm2d(out_channels)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        Y = F.relu(self.bn1(self.conv1(X)))</span><br><span class="line">        Y = self.bn2(self.conv2(Y))</span><br><span class="line">        <span class="keyword">if</span> self.conv3: <span class="comment">#要用1x1卷积层改变原输入的通道数</span></span><br><span class="line">            X = self.conv3(X)</span><br><span class="line">        <span class="keyword">return</span> F.relu(Y + X) <span class="comment">#将原输入与Y相加再激活</span></span><br></pre></td></tr></table></figure>

<p>ResNet的前两层跟之前介绍的GoogLeNet中的一样：在输出通道数为64、步幅为2的7×7卷积层后接步幅为2的3×3的最大池化层。不同之处在于ResNet每个卷积层后增加的批量归一化层。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(</span><br><span class="line">        nn.Conv2d(<span class="number">1</span>, <span class="number">64</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>),</span><br><span class="line">        nn.BatchNorm2d(<span class="number">64</span>),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br><span class="line">        )</span><br></pre></td></tr></table></figure>
<p>GoogLeNet在后面接了4个由Inception块组成的模块。ResNet则使用4个由残差块组成的模块，每个模块使用若干个同样输出通道数的残差块。第一个模块的通道数同输入通道数一致。由于之前已经使用了步幅为2的最大池化层，所以无须减小高和宽。之后的每个模块在第一个残差块里将上一个模块的通道数翻倍，并将高和宽减半。</p>
<p>下面我们来实现这个模块。注意，这里对第一个模块做了特别处理。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">resnet_block</span><span class="params">(in_channels, out_channels, num_residuals, first_block=False)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> first_block:</span><br><span class="line">        <span class="keyword">assert</span> in_channels == out_channels <span class="comment"># 第一个模块的通道数同输入通道数一致</span></span><br><span class="line">    blk = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_residuals):</span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> first_block:</span><br><span class="line">            blk.append(Residual(in_channels, out_channels, use_1x1conv=<span class="literal">True</span>, stride=<span class="number">2</span>))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            blk.append(Residual(out_channels, out_channels))</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(*blk)</span><br></pre></td></tr></table></figure>
<p>接着我们为ResNet加入所有残差块。这里每个模块使用两个残差块。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">net.add_module(<span class="string">"resnet_block1"</span>, resnet_block(<span class="number">64</span>, <span class="number">64</span>, <span class="number">2</span>, first_block=<span class="literal">True</span>))</span><br><span class="line">net.add_module(<span class="string">"resnet_block2"</span>, resnet_block(<span class="number">64</span>, <span class="number">128</span>, <span class="number">2</span>))</span><br><span class="line">net.add_module(<span class="string">"resnet_block3"</span>, resnet_block(<span class="number">128</span>, <span class="number">256</span>, <span class="number">2</span>))</span><br><span class="line">net.add_module(<span class="string">"resnet_block4"</span>, resnet_block(<span class="number">256</span>, <span class="number">512</span>, <span class="number">2</span>))</span><br></pre></td></tr></table></figure>
<p>最后，与GoogLeNet一样，加入全局平均池化层后接上全连接层输出。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net.add_module(<span class="string">"global_avg_pool"</span>, d2l.GlobalAvgPool2d()) <span class="comment"># GlobalAvgPool2d的输出: (Batch, 512, 1, 1)</span></span><br><span class="line">net.add_module(<span class="string">"fc"</span>, nn.Sequential(d2l.FlattenLayer(), nn.Linear(<span class="number">512</span>, <span class="number">10</span>)))</span><br></pre></td></tr></table></figure>
<br>

<h1 id="DenseNet"><a href="#DenseNet" class="headerlink" title="DenseNet"></a>DenseNet</h1><p>稠密连接网络（DenseNet）：<em>通道维上连接</em><br>残差网络（ResNet）：<em>相加</em><br><img src="https://imgvideoforblog.oss-cn-shanghai.aliyuncs.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202020-02-09%20%E4%B8%8B%E5%8D%885.51.05.png" alt="alt"><br>图5.10中将部分前后相邻的运算抽象为模块A和模块B。与ResNet的主要区别在于，DenseNet里模块B的输出不是像ResNet那样和模块A的输出相加，而是在通道维上连结。这样<strong>模块A的输出可以直接传入模块B后面的层</strong>。在这个设计里，模块A直接跟模块B后面的所有层连接在了一起。这也是它被称为“稠密连接”的原因。</p>
<p>DenseNet的主要构建模块是<strong>稠密块（dense block）和过渡层（transition layer）</strong>。前者定义了输入和输出是如何连结的，后者则用来控制通道数，使之不过大。</p>
<h2 id="稠密块"><a href="#稠密块" class="headerlink" title="稠密块"></a>稠密块</h2><p>DenseNet使用了ResNet改良版的“批量归一化、激活和卷积”结构，即稠密块的组成部分“卷积块”，先在conv_block函数里实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn, optim</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">".."</span>) </span><br><span class="line"><span class="keyword">import</span> d2lzh_pytorch <span class="keyword">as</span> d2l</span><br><span class="line">device = torch.device(<span class="string">'cuda'</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">'cpu'</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_block</span><span class="params">(in_channels, out_channels)</span>:</span></span><br><span class="line">    blk = nn.Sequential(nn.BatchNorm2d(in_channels),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.Conv2d(in_channels, out_channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">return</span> blk</span><br></pre></td></tr></table></figure>
<p>这里首先是BN和Activation，其实是因为DenseNet最开始是有卷积层的。</p>
<p>接下来实现稠密块。稠密块就是以这DenseNet的设计逻辑体现，即卷积块之后需要将原输入在通道维进行连接。所以，<em>每块使用相同的输出通道数，但输入通道数是一直增大的。</em></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DenseBlock</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_convs, in_channels, out_channels)</span>:</span></span><br><span class="line">        super(DenseBlock, self).__init__()</span><br><span class="line">        net = []</span><br><span class="line">        <span class="comment">#依次添加卷积块</span></span><br><span class="line">        <span class="comment">#要注意由于“连接”了，所以每个卷积块的输入维度增大</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(num_convs):</span><br><span class="line">            in_c = in_channels + i*out_channels </span><br><span class="line">            net.append(conv_block(in_c, out_channels))</span><br><span class="line">        self.net = nn.ModuleList(net)</span><br><span class="line">        self.out_channels = in_channels + num_convs * out_channels <span class="comment"># 计算最终的输出通道数</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> blk <span class="keyword">in</span> self.net:</span><br><span class="line">            Y = blk(X)</span><br><span class="line">            X = torch.cat((X, Y), dim=<span class="number">1</span>) <span class="comment">#在通道维上连接</span></span><br><span class="line">        <span class="keyword">return</span> X</span><br></pre></td></tr></table></figure>
<p>必须要注意的是，<strong>此稠密块中，串联了多个卷积块，而每个卷积块输出连接后其实又作为“原数据”与下一个卷积块的输出相连接！</strong>比如，我们定义一个有2个输出通道数为10的卷积块。使用通道数为3的输入时，我们会得到通道数为3+2×10=23的输出。（即第一个卷积层之后连接后通道数有13，第二个之后通道数本有10，连接后则为10+13=23个通道的输出）。因此，卷积块的通道数控制了输出通道数相对于输入通道数的增长，因此也被称为增长率（growth rate）。<br><br></p>
<h2 id="过渡层"><a href="#过渡层" class="headerlink" title="过渡层"></a>过渡层</h2><p>由于每个稠密块都会带来通道数的增加，使用过多则会带来过于复杂的模型。过渡层用来控制模型复杂度。它<em>通过1×1卷积层来减小通道数，并使用步幅为2的平均池化层减半高和宽，从而进一步降低模型复杂度。</em></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">transition_block</span><span class="params">(in_channels, out_channels)</span>:</span></span><br><span class="line">    blk = nn.Sequential(</span><br><span class="line">            nn.BatchNorm2d(in_channels), </span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Conv2d(in_channels, out_channels, kernel_size=<span class="number">1</span>), <span class="comment">#1*1卷积层改变通道数</span></span><br><span class="line">            nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)) <span class="comment">#平均池化层步长=2，使高宽减半</span></span><br><span class="line">    <span class="keyword">return</span> blk</span><br></pre></td></tr></table></figure>

<h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>我们来构造DenseNet模型。DenseNet首先使用同ResNet一样的单卷积层和最大池化层。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(</span><br><span class="line">        nn.Conv2d(<span class="number">1</span>, <span class="number">64</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>),</span><br><span class="line">        nn.BatchNorm2d(<span class="number">64</span>), </span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br></pre></td></tr></table></figure>

<p>类似于ResNet接下来使用的4个残差块，DenseNet使用的是4个稠密块。同ResNet一样，我们可以设置每个稠密块使用多少个卷积层。这里我们设成4，从而与上一节的ResNet-18保持一致。稠密块里的卷积层通道数（即增长率）设为32，所以每个稠密块将增加128个通道。</p>
<p>ResNet里通过步幅为2的残差块在每个模块之间减小高和宽。这里我们则使用过渡层来减半高和宽，并减半通道数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">num_channels, growth_rate = <span class="number">64</span>, <span class="number">32</span>  <span class="comment"># num_channels为当前的通道数</span></span><br><span class="line">num_convs_in_dense_blocks = [<span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i, num_convs <span class="keyword">in</span> enumerate(num_convs_in_dense_blocks):</span><br><span class="line">    DB = DenseBlock(num_convs, num_channels, growth_rate)</span><br><span class="line">    net.add_module(<span class="string">"DenseBlosk_%d"</span> % i, DB)</span><br><span class="line">    <span class="comment"># 上一个稠密块的输出通道数</span></span><br><span class="line">    num_channels = DB.out_channels</span><br><span class="line">    <span class="comment"># 在稠密块之间加入通道数减半的过渡层</span></span><br><span class="line">    <span class="keyword">if</span> i != len(num_convs_in_dense_blocks) - <span class="number">1</span>:</span><br><span class="line">        net.add_module(<span class="string">"transition_block_%d"</span> % i, transition_block(num_channels, num_channels // <span class="number">2</span>))</span><br><span class="line">        num_channels = num_channels // <span class="number">2</span></span><br></pre></td></tr></table></figure>
<p>同ResNet一样，最后接上全局池化层和全连接层来输出。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">net.add_module(<span class="string">"BN"</span>, nn.BatchNorm2d(num_channels))</span><br><span class="line">net.add_module(<span class="string">"relu"</span>, nn.ReLU())</span><br><span class="line">net.add_module(<span class="string">"global_avg_pool"</span>, d2l.GlobalAvgPool2d()) <span class="comment"># GlobalAvgPool2d的输出: (Batch, num_channels, 1, 1)</span></span><br><span class="line">net.add_module(<span class="string">"fc"</span>, nn.Sequential(d2l.FlattenLayer(), nn.Linear(num_channels, <span class="number">10</span>)))</span><br></pre></td></tr></table></figure>
<p>我们尝试打印每个子模块的输出维度确保网络无误：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">X = torch.rand((<span class="number">1</span>, <span class="number">1</span>, <span class="number">96</span>, <span class="number">96</span>))</span><br><span class="line"><span class="keyword">for</span> name, layer <span class="keyword">in</span> net.named_children():</span><br><span class="line">    X = layer(X)</span><br><span class="line">    print(name, <span class="string">' output shape:\t'</span>, X.shape)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">0</span>  output shape:     torch.Size([<span class="number">1</span>, <span class="number">64</span>, <span class="number">48</span>, <span class="number">48</span>])</span><br><span class="line"><span class="number">1</span>  output shape:     torch.Size([<span class="number">1</span>, <span class="number">64</span>, <span class="number">48</span>, <span class="number">48</span>])</span><br><span class="line"><span class="number">2</span>  output shape:     torch.Size([<span class="number">1</span>, <span class="number">64</span>, <span class="number">48</span>, <span class="number">48</span>])</span><br><span class="line"><span class="number">3</span>  output shape:     torch.Size([<span class="number">1</span>, <span class="number">64</span>, <span class="number">24</span>, <span class="number">24</span>])</span><br><span class="line">DenseBlosk_0  output shape:     torch.Size([<span class="number">1</span>, <span class="number">192</span>, <span class="number">24</span>, <span class="number">24</span>])</span><br><span class="line">transition_block_0  output shape:     torch.Size([<span class="number">1</span>, <span class="number">96</span>, <span class="number">12</span>, <span class="number">12</span>])</span><br><span class="line">DenseBlosk_1  output shape:     torch.Size([<span class="number">1</span>, <span class="number">224</span>, <span class="number">12</span>, <span class="number">12</span>])</span><br><span class="line">transition_block_1  output shape:     torch.Size([<span class="number">1</span>, <span class="number">112</span>, <span class="number">6</span>, <span class="number">6</span>])</span><br><span class="line">DenseBlosk_2  output shape:     torch.Size([<span class="number">1</span>, <span class="number">240</span>, <span class="number">6</span>, <span class="number">6</span>])</span><br><span class="line">transition_block_2  output shape:     torch.Size([<span class="number">1</span>, <span class="number">120</span>, <span class="number">3</span>, <span class="number">3</span>])</span><br><span class="line">DenseBlosk_3  output shape:     torch.Size([<span class="number">1</span>, <span class="number">248</span>, <span class="number">3</span>, <span class="number">3</span>])</span><br><span class="line">BN  output shape:     torch.Size([<span class="number">1</span>, <span class="number">248</span>, <span class="number">3</span>, <span class="number">3</span>])</span><br><span class="line">relu  output shape:     torch.Size([<span class="number">1</span>, <span class="number">248</span>, <span class="number">3</span>, <span class="number">3</span>])</span><br><span class="line">global_avg_pool  output shape:     torch.Size([<span class="number">1</span>, <span class="number">248</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">fc  output shape:     torch.Size([<span class="number">1</span>, <span class="number">10</span>])</span><br></pre></td></tr></table></figure>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/02/09/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" data-id="ck6ewh7gm00003d6cfksd56v0"
         class="article-share-link">Share</a>
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag">动手学深度学习</a></li></ul>

    </footer>

  </div>

  
    
  <nav class="article-nav">
    
      <a href="/2020/02/12/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" class="article-nav-link">
        <strong class="article-nav-caption">Newer posts</strong>
        <div class="article-nav-title">
          
            循环神经网络
          
        </div>
      </a>
    
    
      <a href="/2020/02/06/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97/" class="article-nav-link">
        <strong class="article-nav-caption">Olde posts</strong>
        <div class="article-nav-title">深度学习计算</div>
      </a>
    
  </nav>


  

  
    
  <div class="gitalk" id="gitalk-container"></div>
  
<link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">

  
<script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>

  
<script src="https://cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.min.js"></script>

  <script type="text/javascript">
    var gitalk = new Gitalk({
      clientID: 'dd9dd0a01eb6aea2b4b2',
      clientSecret: 'f10df8d5a64e14405c16d484ff17f526f1ff2c21',
      repo: 'gitalk_blog',
      owner: 'JosephLZD',
      admin: ['JosephLZD'],
      // id: location.pathname,      // Ensure uniqueness and length less than 50
      id: md5(location.pathname),
      distractionFreeMode: false,  // Facebook-like distraction free mode
      pagerDirection: 'last'
    })

  gitalk.render('gitalk-container')
  </script>

  

</article>



</section>
  <footer class="footer">
  <div class="outer">
    <div class="float-right">
      <ul class="list-inline">
  
    <li><i class="fe fe-smile-alt"></i> <span id="busuanzi_value_site_uv"></span></li>
  
</ul>
    </div>
    <ul class="list-inline">
      <li>&copy; 2020 Blog_JosephLZD</li>
      <li>Powered by <a href="http://hexo.io/" target="_blank">Hexo</a></li>
    </ul>
  </div>
</footer>

</main>

<aside class="sidebar sidebar-specter">
  
    <button class="navbar-toggle"></button>
<nav class="navbar">
  
    <div class="logo">
      <a href="/"><img src="/images/hexo.svg" alt="Blog_JosephLZD"></a>
    </div>
  
  <ul class="nav nav-main">
    
      <li class="nav-item">
        <a class="nav-item-link" href="/">Home</a>
      </li>
    
      <li class="nav-item">
        <a class="nav-item-link" href="/archives">Archive</a>
      </li>
    
      <li class="nav-item">
        <a class="nav-item-link" href="/gallery">Gallery</a>
      </li>
    
      <li class="nav-item">
        <a class="nav-item-link" href="/about">About</a>
      </li>
    
    <li class="nav-item">
      <a class="nav-item-link nav-item-search" title="搜索">
        <i class="fe fe-search"></i>
        Search
      </a>
    </li>
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      <div class="totop" id="totop">
  <i class="fe fe-rocket"></i>
</div>
    </li>
    <li class="nav-item">
      
        <a class="nav-item-link" target="_blank" href="/atom.xml" title="RSS Feed">
          <i class="fe fe-feed"></i>
        </a>
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
  </aside>
  
<script src="/js/jquery-2.0.3.min.js"></script>


<script src="/js/jquery.justifiedGallery.min.js"></script>


<script src="/js/lazyload.min.js"></script>


<script src="/js/busuanzi-2.3.pure.min.js"></script>


  
<script src="/fancybox/jquery.fancybox.min.js"></script>




  
<script src="/js/tocbot.min.js"></script>

  <script>
    // Tocbot_v4.7.0  http://tscanlin.github.io/tocbot/
    tocbot.init({
      tocSelector: '.tocbot',
      contentSelector: '.article-entry',
      headingSelector: 'h1, h2, h3, h4, h5, h6',
      hasInnerContainers: true,
      scrollSmooth: true,
      positionFixedSelector: '.tocbot',
      positionFixedClass: 'is-position-fixed',
      fixedSidebarOffset: 'auto',
    });
  </script>



<script src="/js/ocean.js"></script>


</body>
</html>