<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  
  
  
    <meta name="description" content="Ain&#39;t no mountain high enough">
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <title>
    Glove及词向量应用 |
    
    Blog_JosephLZD</title>
  
    <link rel="shortcut icon" href="/favicon.ico">
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<script src="/js/pace.min.js"></script>

<meta name="generator" content="Hexo 4.1.1"></head>

<body>
<main class="content">
  <section class="outer">
  

<article id="post-Glove及词向量应用" class="article article-type-post" itemscope itemprop="blogPost" data-scroll-reveal>
  
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Glove及词向量应用
    </h1>
  
  




      </header>
    

    
      <div class="article-meta">
        <a href="/2020/02/17/Glove%E5%8F%8A%E8%AF%8D%E5%90%91%E9%87%8F%E5%BA%94%E7%94%A8/" class="article-date">
  <time datetime="2020-02-17T02:45:33.000Z" itemprop="datePublished">2020-02-17</time>
</a>
        
      </div>
    

    
      
    <div class="tocbot"></div>





    

    <div class="article-entry" itemprop="articleBody">
      


      

      
        <p>《动手学深度学习pytorch版》“自然语言处理”学习笔记PART2，包括子词嵌入FastText、全局向量的词嵌入Glove，以及运用torchtext工具包即现成的词向量求近义词和类比词。</p>
<a id="more"></a>

<h1 id="子词嵌入"><a href="#子词嵌入" class="headerlink" title="子词嵌入"></a>子词嵌入</h1><p>在word2vec中，我们并没有直接利用构词学中的信息。无论是在跳字模型还是连续词袋模型中，我们都将形态不同的单词用不同的向量来表示。例如，“dog”和“dogs”分别用两个不同的向量表示，而模型中并未直接表达这两个向量之间的关系。</p>
<p>鉴于此，fastText提出了子词嵌入（subword embedding）的方法，从而试图将构词信息引入word2vec中的跳字模型 [1]。</p>
<p><img src="https://imgvideoforblog.oss-cn-shanghai.aliyuncs.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202020-02-16%20%E4%B8%8B%E5%8D%888.45.26.png" alt="alt"><br>[1] Bojanowski, P., Grave, E., Joulin, A., &amp; Mikolov, T. (2016). Enriching word vectors with subword information. arXiv preprint arXiv:1607.04606.</p>
<Br>

<h1 id="Glove"><a href="#Glove" class="headerlink" title="Glove"></a>Glove</h1><p>首先回顾一下之前实现的word2vec中的skip-gram，损失函数是交叉熵，无论是softmax交叉熵还是负采样处理后的sigmoid交叉熵，都是以一个one-hot向量作为label来标识哪些是该中心词对应的背景词，从而区分开正例和负例。</p>
<p>但Glove这里想换种方法，其实就是换种损失函数，换成“平方损失函数”。</p>
<p>也就是说，输出计算仍然是两个词向量乘积uTv（会取个对数），但是不是用交叉熵采用的one-hot的label，而是用<strong>在本数据集中，各背景词出现在中心词周围的次数</strong> 作为y。<br>从而，该平方损失衡量的是uTv和uv相邻的次数。</p>
<p>注：xij表示词wj出现在词wi周围，作为其背景词的次数。而这些次数都是在整个数据集中预先数好了的，所以Glove叫全局向量。<br><img src="https://imgvideoforblog.oss-cn-shanghai.aliyuncs.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202020-02-16%20%E4%B8%8B%E5%8D%889.35.24.png" alt="alt"><br><Br></p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>1.在有些情况下，交叉熵损失函数有劣势。GloVe模型采用了平方损失，并通过词向量拟合预先基于整个数据集计算得到的全局统计信息。</p>
<p>2.因为xij==xji，即条件概率对称，所以任意词的中心词向量和背景词向量在GloVe模型中是等价的。<br>但由于初始化值的不同，同一个词最终学习到的两组词向量可能不同。当学习得到所有词向量以后，GloVe模型使用<strong>中心词向量与背景词向量之和</strong>作为该词的最终词向量。<br><br></p>
<h1 id="工具包"><a href="#工具包" class="headerlink" title="工具包"></a>工具包</h1><p>基于PyTorch的关于自然语言处理的常用包有官方的torchtext以及第三方的pytorch-nlp等等。你可以使用pip很方便地按照它们，例如命令行执行</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install torchtext</span><br></pre></td></tr></table></figure>
<p>本节我们使用torchtext进行练习。下面查看它目前提供的预训练词嵌入的名称。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchtext.vocab <span class="keyword">as</span> vocab</span><br><span class="line"></span><br><span class="line">vocab.pretrained_aliases.keys()</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dict_keys([<span class="string">'charngram.100d'</span>, <span class="string">'fasttext.en.300d'</span>, <span class="string">'fasttext.simple.300d'</span>, <span class="string">'glove.42B.300d'</span>, <span class="string">'glove.840B.300d'</span>, <span class="string">'glove.twitter.27B.25d'</span>, <span class="string">'glove.twitter.27B.50d'</span>, <span class="string">'glove.twitter.27B.100d'</span>, <span class="string">'glove.twitter.27B.200d'</span>, <span class="string">'glove.6B.50d'</span>, <span class="string">'glove.6B.100d'</span>, <span class="string">'glove.6B.200d'</span>, <span class="string">'glove.6B.300d'</span>])</span><br></pre></td></tr></table></figure>
<p>下面查看查看该glove词嵌入提供了哪些预训练的模型。每个模型的词向量维度可能不同，或是在不同数据集上预训练得到的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[key <span class="keyword">for</span> key <span class="keyword">in</span> vocab.pretrained_aliases.keys()</span><br><span class="line">        <span class="keyword">if</span> <span class="string">"glove"</span> <span class="keyword">in</span> key]</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">'glove.42B.300d'</span>,</span><br><span class="line"> <span class="string">'glove.840B.300d'</span>,</span><br><span class="line"> <span class="string">'glove.twitter.27B.25d'</span>,</span><br><span class="line"> <span class="string">'glove.twitter.27B.50d'</span>,</span><br><span class="line"> <span class="string">'glove.twitter.27B.100d'</span>,</span><br><span class="line"> <span class="string">'glove.twitter.27B.200d'</span>,</span><br><span class="line"> <span class="string">'glove.6B.50d'</span>,</span><br><span class="line"> <span class="string">'glove.6B.100d'</span>,</span><br><span class="line"> <span class="string">'glove.6B.200d'</span>,</span><br><span class="line"> <span class="string">'glove.6B.300d'</span>]</span><br></pre></td></tr></table></figure>
<p>预训练的GloVe模型的命名规范大致是“模型.（数据集.）数据集词数.词向量维度”。更多信息可以参考GloVe和fastText的项目网站[1,2]。下面我们使用基于维基百科子集预训练的50维GloVe词向量。第一次创建预训练词向量实例时会自动下载相应的词向量到cache指定文件夹（默认为.vector_cache），因此需要联网。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cache_dir = <span class="string">"/Users/tangshusen/Datasets/glove"</span></span><br><span class="line"><span class="comment"># glove = vocab.pretrained_aliases["glove.6B.50d"](cache=cache_dir)</span></span><br><span class="line">glove = vocab.GloVe(name=<span class="string">'6B'</span>, dim=<span class="number">50</span>, cache=cache_dir) <span class="comment"># 与上面等价</span></span><br></pre></td></tr></table></figure>
<p>返回的实例主要有以下三个属性：<br>stoi: 词到索引的字典：<br>itos: 一个列表，索引到词的映射；<br>vectors: 词向量。</p>
<p>我们可以通过词来获取它在词典中的索引，也可以通过索引获取词。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">glove.stoi[<span class="string">'beautiful'</span>], glove.itos[<span class="number">3366</span>] <span class="comment"># (3366, 'beautiful')</span></span><br></pre></td></tr></table></figure>
<br>

<h1 id="求近义词"><a href="#求近义词" class="headerlink" title="求近义词"></a>求近义词</h1><p>用余弦相似度来搜索近义词：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#计算topk个余弦相似度最大的词</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">knn</span><span class="params">(W, x, k)</span>:</span></span><br><span class="line">    <span class="comment"># 添加的1e-9是为了数值稳定性</span></span><br><span class="line">    cos = torch.matmul(W, x.view((<span class="number">-1</span>,))) / (</span><br><span class="line">        (torch.sum(W * W, dim=<span class="number">1</span>) + <span class="number">1e-9</span>).sqrt() * torch.sum(x * x).sqrt())</span><br><span class="line">    _, topk = torch.topk(cos, k=k)</span><br><span class="line">    topk = topk.cpu().numpy()</span><br><span class="line">    <span class="keyword">return</span> topk, [cos[i].item() <span class="keyword">for</span> i <span class="keyword">in</span> topk]</span><br></pre></td></tr></table></figure>
<p>然后，我们通过预训练词向量实例embed来搜索近义词。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_similar_tokens</span><span class="params">(query_token, k, embed)</span>:</span></span><br><span class="line">    topk, cos = knn(embed.vectors, embed.vectors[embed.stoi[query_token]], k+<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">for</span> i, c <span class="keyword">in</span> zip(topk[<span class="number">1</span>:], cos[<span class="number">1</span>:]): <span class="comment">#除去输入词</span></span><br><span class="line">        print(<span class="string">'cosine sim=%.3f: %s'</span> % (c, (embed.itos[i])))</span><br></pre></td></tr></table></figure>
<p>已创建的预训练词向量实例glove_6b50d的词典中含40万个词和1个特殊的未知词。除去输入词和未知词，我们从中搜索与“chip”语义最相近的3个词。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">get_similar_tokens(<span class="string">'chip'</span>, <span class="number">3</span>, glove)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cosine sim=<span class="number">0.856</span>: chips</span><br><span class="line">cosine sim=<span class="number">0.749</span>: intel</span><br><span class="line">cosine sim=<span class="number">0.749</span>: electronics</span><br></pre></td></tr></table></figure>
<br>

<h1 id="求类比词"><a href="#求类比词" class="headerlink" title="求类比词"></a>求类比词</h1><p>除了求近义词以外，我们还可以使用预训练词向量求词与词之间的类比关系。例如，“man”（男人）: “woman”（女人）:: “son”（儿子） : “daughter”（女儿）是一个类比例子：“man”之于“woman”相当于“son”之于“daughter”。求类比词问题可以定义为：对于类比关系中的4个词 a : b :: c : d，给定前3个词a、b和c，求d。设词w的词向量为vec(w)。</p>
<p>因为a:b == c:d，所以相当于 b-a == d-c，所以c+b-a==d<br>求类比词d的思路是，搜索与<code>vec(c)+vec(b)−vec(a)</code>的结果向量最相似的词向量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_analogy</span><span class="params">(token_a, token_b, token_c, embed)</span>:</span></span><br><span class="line">    vecs = [embed.vectors[embed.stoi[t]] </span><br><span class="line">                <span class="keyword">for</span> t <span class="keyword">in</span> [token_a, token_b, token_c]]</span><br><span class="line">    x = vecs[<span class="number">1</span>] - vecs[<span class="number">0</span>] + vecs[<span class="number">2</span>]</span><br><span class="line">    topk, cos = knn(embed.vectors, x, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> embed.itos[topk[<span class="number">0</span>]]</span><br></pre></td></tr></table></figure>
<p>验证一下“男-女”类比。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">get_analogy(<span class="string">'man'</span>, <span class="string">'woman'</span>, <span class="string">'son'</span>, glove) <span class="comment"># 'daughter'</span></span><br></pre></td></tr></table></figure>
<p>“动词一般时-动词过去时”类比：“do”（做）之于“did”（做过）相当于“go”（去）之于什么？答案应该是“went”（去过）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">get_analogy(<span class="string">'do'</span>, <span class="string">'did'</span>, <span class="string">'go'</span>, glove) <span class="comment"># 'went'</span></span><br></pre></td></tr></table></figure>
<Br>

<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>[1] GloVe项目网站。 <a href="https://nlp.stanford.edu/projects/glove/" target="_blank" rel="noopener">https://nlp.stanford.edu/projects/glove/</a></p>
<p>[2] fastText项目网站。 <a href="https://fasttext.cc/" target="_blank" rel="noopener">https://fasttext.cc/</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/02/17/Glove%E5%8F%8A%E8%AF%8D%E5%90%91%E9%87%8F%E5%BA%94%E7%94%A8/" data-id="ck6puzb220000xb6cc12petui"
         class="article-share-link">Share</a>
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag">动手学深度学习</a></li></ul>

    </footer>

  </div>

  
    
  <nav class="article-nav">
    
    
      <a href="/2020/02/16/Word2Vec/" class="article-nav-link">
        <strong class="article-nav-caption">Olde posts</strong>
        <div class="article-nav-title">Word2Vec</div>
      </a>
    
  </nav>


  

  
    
  <div class="gitalk" id="gitalk-container"></div>
  
<link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">

  
<script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>

  
<script src="https://cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.min.js"></script>

  <script type="text/javascript">
    var gitalk = new Gitalk({
      clientID: 'dd9dd0a01eb6aea2b4b2',
      clientSecret: 'f10df8d5a64e14405c16d484ff17f526f1ff2c21',
      repo: 'gitalk_blog',
      owner: 'JosephLZD',
      admin: ['JosephLZD'],
      // id: location.pathname,      // Ensure uniqueness and length less than 50
      id: md5(location.pathname),
      distractionFreeMode: false,  // Facebook-like distraction free mode
      pagerDirection: 'last'
    })

  gitalk.render('gitalk-container')
  </script>

  

</article>



</section>
  <footer class="footer">
  <div class="outer">
    <div class="float-right">
      <ul class="list-inline">
  
    <li><i class="fe fe-smile-alt"></i> <span id="busuanzi_value_site_uv"></span></li>
  
</ul>
    </div>
    <ul class="list-inline">
      <li>&copy; 2020 Blog_JosephLZD</li>
      <li>Powered by <a href="http://hexo.io/" target="_blank">Hexo</a></li>
    </ul>
  </div>
</footer>

</main>

<aside class="sidebar sidebar-specter">
  
    <button class="navbar-toggle"></button>
<nav class="navbar">
  
    <div class="logo">
      <a href="/"><img src="/images/hexo.svg" alt="Blog_JosephLZD"></a>
    </div>
  
  <ul class="nav nav-main">
    
      <li class="nav-item">
        <a class="nav-item-link" href="/">Home</a>
      </li>
    
      <li class="nav-item">
        <a class="nav-item-link" href="/archives">Archive</a>
      </li>
    
      <li class="nav-item">
        <a class="nav-item-link" href="/gallery">Gallery</a>
      </li>
    
      <li class="nav-item">
        <a class="nav-item-link" href="/about">About</a>
      </li>
    
    <li class="nav-item">
      <a class="nav-item-link nav-item-search" title="搜索">
        <i class="fe fe-search"></i>
        Search
      </a>
    </li>
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      <div class="totop" id="totop">
  <i class="fe fe-rocket"></i>
</div>
    </li>
    <li class="nav-item">
      
        <a class="nav-item-link" target="_blank" href="/atom.xml" title="RSS Feed">
          <i class="fe fe-feed"></i>
        </a>
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
  </aside>
  
<script src="/js/jquery-2.0.3.min.js"></script>


<script src="/js/jquery.justifiedGallery.min.js"></script>


<script src="/js/lazyload.min.js"></script>


<script src="/js/busuanzi-2.3.pure.min.js"></script>


  
<script src="/fancybox/jquery.fancybox.min.js"></script>




  
<script src="/js/tocbot.min.js"></script>

  <script>
    // Tocbot_v4.7.0  http://tscanlin.github.io/tocbot/
    tocbot.init({
      tocSelector: '.tocbot',
      contentSelector: '.article-entry',
      headingSelector: 'h1, h2, h3, h4, h5, h6',
      hasInnerContainers: true,
      scrollSmooth: true,
      positionFixedSelector: '.tocbot',
      positionFixedClass: 'is-position-fixed',
      fixedSidebarOffset: 'auto',
    });
  </script>



<script src="/js/ocean.js"></script>


</body>
</html>